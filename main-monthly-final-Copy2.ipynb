{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41cee3da-2f80-48bc-b970-1ffd10fe2c2c",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime\n",
    "import time\n",
    "import statsmodels.api as sm\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from my_ipca import InstrumentedPCA\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "import copy\n",
    "from scipy.stats import ks_2samp\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import importlib\n",
    "import sys\n",
    "from collections import defaultdict\n",
    "from IPython.display import display, Latex\n",
    "from cvxopt import matrix, solvers, blas\n",
    "import pickle\n",
    "from pmdarima import auto_arima\n",
    "from statsmodels.stats.diagnostic import acorr_ljungbox\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.svm import SVR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66ebc1b5-f68d-450b-9c1b-70bed9cbee5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1673 coins with OHLC and volume\n",
    "directory_path = 'C:/Users/86152/Desktop/jupyter code/Dissertation/TokenDataFilteredNew'  ,
    "file_names = os.listdir(directory_path)\n",
    "csv_files = [f for f in file_names if f.endswith('.csv')]  ,
    "\n",
    "dataframes = []  # 用来储存每个csv文件的DataFrame\n",
    "for csv_file in csv_files:\n",
    "    df = pd.read_csv(os.path.join(directory_path, csv_file))\n",
    "    df['Currency'] = csv_file[:-4]  # 添加一个新列来表示加密货币的名字\n",
    "    dataframes.append(df)\n",
    "\n",
    "combined_df = pd.concat(dataframes)  # 将所有的DataFrame合并为一个\n",
    "\n",
    "combined_df.set_index(['Currency', 'time'], inplace=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "207e6fc9-a1d0-4788-a082-0b6ded8e5cb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# change to pandas timestamp\n",
    "combined_df.reset_index(level='time', inplace=True)  # 重置 'time' 索引\n",
    "combined_df['time'] = pd.to_datetime(combined_df['time'], unit='s')  # 转换 Unix 时间戳为可读日期时间\n",
    "combined_df.set_index('time', append=True, inplace=True)  # 将 'time' 列重新设为索引\n",
    "\n",
    "combined_df.sort_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f71fce0d-eed0-44f5-aeec-305f3189ca50",
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_df = pd.read_csv('^TNX.csv')\n",
    "\n",
    "# 将'Date'列转换为日期类型并设置为索引\n",
    "rf_df['Date'] = pd.to_datetime(rf_df['Date'])\n",
    "# 将Date设为索引\n",
    "rf_df.set_index('Date', inplace=True)\n",
    "\n",
    "# 使日期索引连续\n",
    "rf_df = rf_df.asfreq('D')\n",
    "\n",
    "# 使用线性插值填充缺失的数据\n",
    "rf_df['Close'] = rf_df['Close'].interpolate(method='linear')\n",
    "rf_df['daily_return'] = rf_df['Close'].pct_change()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff1868c1-499b-404d-b1c7-c094f954716e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Extract unique dates from combined_df\n",
    "unique_dates = combined_df.index.get_level_values('time').unique()\n",
    "\n",
    "# 2. Use these dates to filter rf_df\n",
    "rf_df = rf_df[rf_df.index.isin(unique_dates)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0f96ece-79d2-40ca-a429-da05e2f837a3",
   "metadata": {},
   "source": [
    "----\n",
    "### Data pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35259ede-c8a7-437a-bdbb-d06db78ebd6b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "'''columns_to_check = ['open', 'close', 'high', 'low', 'volumeto', 'volumefrom', 'current_supply']\n",
    "\n",
    "def find_consecutive_zeros(data):\n",
    "    \"\"\"Function to identify columns with three consecutive zeros after the first non-zero 'close'.\"\"\"\n",
    "    # Find the index for the first non-zero 'close'\n",
    "    first_non_zero_idx = data['close'].ne(0).idxmax()\n",
    "    data = data.loc[first_non_zero_idx:]\n",
    "    \n",
    "    consecutive_zero_columns = []\n",
    "    for column in columns_to_check:\n",
    "        mask = (data[column] == 0) | (data[column].isna())\n",
    "        consecutive_mask = mask & mask.shift(-1) & mask.shift(-2)\n",
    "        if consecutive_mask.any():\n",
    "            consecutive_zero_columns.append(column)\n",
    "    return consecutive_zero_columns\n",
    "\n",
    "# Apply the function to each currency group\n",
    "currencies_with_consecutive_zeros = combined_df.groupby(level='Currency').apply(find_consecutive_zeros)\n",
    "\n",
    "# Identify currencies to remove\n",
    "currencies_to_remove = currencies_with_consecutive_zeros[currencies_with_consecutive_zeros.apply(len) > 0].index.tolist()\n",
    "\n",
    "# Drop these currencies from the original DataFrame\n",
    "combined_df = combined_df.drop(currencies_to_remove, level='Currency')'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24cf4d1f-7357-41e1-9914-01fa00228344",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_check = ['high', 'low', 'open', 'volumefrom', 'volumeto', 'close', 'new_addresses', 'active_addresses', 'current_supply']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2a9016b-877e-498b-9ed8-cd1843f6a47f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def trim_dataframe(df1):\n",
    "    df = df1.copy()\n",
    "    \n",
    "    # 初始化时间点为None\n",
    "    first_valid_date = None\n",
    "    \n",
    "    # 遍历每一列\n",
    "    for col in columns_to_check:\n",
    "        # 找到第一个非NaN非0的时间点\n",
    "        first_valid = df.loc[(~df[col].isna()) & (df[col] != 0), col].index[0] if not df.loc[(~df[col].isna()) & (df[col] != 0), col].empty else None\n",
    "\n",
    "        if first_valid_date is None or (first_valid and first_valid > first_valid_date):\n",
    "            first_valid_date = first_valid\n",
    "\n",
    "    # 使用找到的时间点裁剪DataFrame\n",
    "    if first_valid_date:\n",
    "        # 获取DataFrame的第一个时间点\n",
    "        first_date = df.index.min()\n",
    "\n",
    "        # 找到first_valid_date之前的时间点\n",
    "        previous_date = df.loc[:first_valid_date].index[-2] if first_valid_date != first_date else None\n",
    "\n",
    "        if previous_date:\n",
    "            df.loc[:previous_date] = np.nan\n",
    "\n",
    "    return df\n",
    "\n",
    "combined_df = combined_df.groupby(level='Currency', group_keys=False).apply(trim_dataframe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15e30b7e-f9ff-43f2-8925-154d248e2134",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "num_currencies = combined_df.index.get_level_values('Currency').nunique()\n",
    "print(num_currencies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68655e78-794a-4ebf-aefe-807062582d4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义一个函数，用于检查每个组中的缺失值\n",
    "def filter_func(group):\n",
    "    volumefrom_zero_count = (group['volumefrom'] == 0).sum()\n",
    "    return  volumefrom_zero_count < 1000\n",
    "\n",
    "# 应用过滤器\n",
    "combined_df = combined_df.groupby('Currency').filter(filter_func)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "592502e7-796d-4657-82b0-aa576cf79d3e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "num_currencies = combined_df.index.get_level_values('Currency').nunique()\n",
    "print(num_currencies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3242dc9-544c-40ca-8d8a-0921b8a8bb4a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def trim_dataframe(df1):\n",
    "    df = df1.copy()\n",
    "    \n",
    "    # 初始化时间点为None\n",
    "    last_zero_date = None\n",
    "    \n",
    "    # 找到'close'列中最后一个0值的时间点\n",
    "    last_zero_date = df.loc[df['close'] == 0, 'close'].index[-1] if not df.loc[df['close'] == 0, 'close'].empty else None\n",
    "\n",
    "    # 使用找到的时间点裁剪DataFrame\n",
    "    if last_zero_date is not None:\n",
    "        df.loc[:last_zero_date] = np.nan\n",
    "\n",
    "    return df\n",
    "\n",
    "# 假设combined_df是一个具有多层索引（MultiIndex）的DataFrame，其中第一层是货币名称（'Currency'）\n",
    "combined_df = combined_df.groupby(level='Currency', group_keys=False).apply(trim_dataframe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ceb07cc-a15c-40b3-ab60-1fe62d833c4c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def replace_30_consecutive_zeros_with_nan(df):\n",
    "    for col in ['active_addresses',  'new_addresses']:\n",
    "        series = df[col]\n",
    "        first_valid_index = series.first_valid_index()\n",
    "        last_valid_index = series.last_valid_index()\n",
    "\n",
    "        if first_valid_index is None or last_valid_index is None:\n",
    "            continue\n",
    "\n",
    "        series1 = series.loc[first_valid_index:last_valid_index].copy()\n",
    "        zero_count = 0\n",
    "        zero_start_index = None\n",
    "        next_index = None\n",
    "\n",
    "        for idx, val in series1.iteritems():\n",
    "            if val == 0:\n",
    "                zero_count += 1\n",
    "                if zero_start_index is None:\n",
    "                    zero_start_index = idx\n",
    "            else:\n",
    "                next_index = idx\n",
    "                if zero_count >= 7 and df.loc[next_index, col]<10:\n",
    "                    df.loc[zero_start_index:next_index, col] = np.nan\n",
    "                    \n",
    "                zero_count = 0\n",
    "                zero_start_index = None\n",
    "                \n",
    "        if zero_count:\n",
    "            df.loc[zero_start_index:last_valid_index, col] = np.nan\n",
    "            \n",
    "    return df\n",
    "\n",
    "# 假设 combined_df 是你的双索引DataFrame\n",
    "combined_df = combined_df.groupby('Currency', group_keys=False).apply(replace_30_consecutive_zeros_with_nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da2c2648-f074-4a68-b3fe-39372bdaf099",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def replace_30_consecutive_zeros_with_nan(df):\n",
    "    for col in ['volumefrom']:\n",
    "        series = df[col]\n",
    "        first_valid_index = series.first_valid_index()\n",
    "        last_valid_index = series.last_valid_index()\n",
    "\n",
    "        if first_valid_index is None or last_valid_index is None:\n",
    "            continue\n",
    "\n",
    "        series1 = series.loc[first_valid_index:last_valid_index].copy()\n",
    "        zero_count = 0\n",
    "        zero_start_index = None\n",
    "\n",
    "        for idx, val in series1.iteritems():\n",
    "            if val == 0:\n",
    "                zero_count += 1\n",
    "                if zero_start_index is None:\n",
    "                    zero_start_index = idx\n",
    "            else:\n",
    "                if zero_count >= 30:\n",
    "                    df.loc[zero_start_index:, :] = np.nan\n",
    "                    break  # No need to continue for this column\n",
    "                zero_count = 0\n",
    "                zero_start_index = None\n",
    "\n",
    "        if zero_count:\n",
    "            df.loc[zero_start_index:last_valid_index, :] = np.nan\n",
    "            \n",
    "    return df\n",
    "\n",
    "# 假设 combined_df 是你的双索引DataFrame\n",
    "combined_df = combined_df.groupby('Currency', group_keys=False).apply(replace_30_consecutive_zeros_with_nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8d108b9-11e4-4fc6-9849-cd69cebb4a05",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "num_currencies = combined_df.index.get_level_values('Currency').nunique()\n",
    "print(num_currencies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8915b3f3-4294-47c8-814e-8c014184e947",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def check_time_overlap(df):\n",
    "    last_non_nan_current_supply = df.loc[~df['current_supply'].isna(), 'current_supply'].index.get_level_values('time').max()\n",
    "    first_non_nan_volumetotal = df.loc[~df['volumefrom'].isna(), 'volumefrom'].index.get_level_values('time').min()\n",
    "\n",
    "    if last_non_nan_current_supply is None or first_non_nan_volumetotal is None:\n",
    "        return False\n",
    "\n",
    "    return last_non_nan_current_supply >= first_non_nan_volumetotal\n",
    "\n",
    "# 假设combined_df是您的DataFrame，其中包含多层索引，第一层是'Currency'\n",
    "valid_currencies = combined_df.groupby(level='Currency', group_keys=False).apply(check_time_overlap)\n",
    "\n",
    "# 仅保留那些时间有交叉的货币\n",
    "valid_currencies = valid_currencies[valid_currencies].index.tolist()\n",
    "combined_df = combined_df.loc[combined_df.index.get_level_values('Currency').isin(valid_currencies)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be8d1acf-c6e5-4058-bde8-0e05298d63de",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "num_currencies = combined_df.index.get_level_values('Currency').nunique()\n",
    "print(num_currencies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "577c7806-817e-413b-82d1-133dc5a854c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def interpolate_zeros(series):\n",
    "    # Find the last valid index which is not 0 and not NaN\n",
    "    first_valid_index = series[series != 0].first_valid_index()\n",
    "    \n",
    "    # If there's no such index, return the series as is\n",
    "    if first_valid_index is None:\n",
    "        return series\n",
    "\n",
    "    # Get the integer location of this index\n",
    "    first_index = series.index.get_loc(first_valid_index)\n",
    "    \n",
    "    # Create a new series limited to this index\n",
    "    limited_series = series.iloc[first_index:]\n",
    "    \n",
    "    limited_series[limited_series.index[limited_series == 0]] = np.nan\n",
    "\n",
    "    # Interpolate the new series\n",
    "    interpolated_series = limited_series.interpolate()\n",
    "    interpolated_series.fillna(method='bfill', inplace=True)\n",
    "    interpolated_series.fillna(method='ffill', inplace=True)\n",
    "    \n",
    "    # Update the original series\n",
    "    series.iloc[first_index:] = interpolated_series\n",
    "\n",
    "    return series\n",
    "\n",
    "# 对每个货币和每一列进行处理\n",
    "for col in ['current_supply']:\n",
    "    combined_df[col] = combined_df.groupby('Currency', group_keys=False)[col].apply(interpolate_zeros)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3935ff93-72f2-479c-a3f3-cfbb391554f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_50_currencies_df = combined_df\n",
    "top_50_currencies_df = top_50_currencies_df.sort_index()\n",
    "top_50_currencies_df.index.get_level_values('Currency').unique()\n",
    "top_50_currencies_df['mark_cap'] = top_50_currencies_df['close'] * top_50_currencies_df['current_supply']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "778769b4-7c9d-4293-9f63-017f128e3ad9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_weight(x):\n",
    "    result = pd.Series(0, index=x.index)\n",
    "    valid_data = x.dropna()\n",
    "    result.loc[valid_data.index] = valid_data / valid_data.sum()\n",
    "    return result\n",
    "\n",
    "top_50_currencies_df['weight'] = top_50_currencies_df.groupby(level='time', group_keys=False)['mark_cap'].transform(compute_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7065155-b342-4446-8eb9-aff89e3fa950",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "num_currencies = top_50_currencies_df.index.get_level_values('Currency').nunique()\n",
    "print(num_currencies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2275a4b5-69d1-42ff-9043-c41671ecf78f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 创建一个新列\"volume_market_cap_ratio\"，包含交易量(volume)和市值(market capitalization)的比率\n",
    "top_50_currencies_df['volume_market_cap_ratio'] = top_50_currencies_df['volumefrom'] / top_50_currencies_df['current_supply']\n",
    "\n",
    "# 筛选出volume_market_cap_ratio大于1的货币\n",
    "filtered_df = top_50_currencies_df[top_50_currencies_df['volume_market_cap_ratio'] > 1]\n",
    "\n",
    "# 对货币进行分组，并统计每种货币大于1的volume_market_cap_ratio出现的次数\n",
    "grouped = filtered_df.groupby('Currency', group_keys=False).size()\n",
    "\n",
    "# 筛选出出现次数大于1的货币\n",
    "invalid_currencies = grouped[grouped > 1].index.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaf64eeb-0de6-446a-ba9c-c60977373172",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "top_50_currencies_df.loc[top_50_currencies_df['volume_market_cap_ratio'] > 1, 'volumeto'] = 1e-10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "973b6d50-7f06-4abd-be07-6a46097c9807",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "top_50_currencies_df.loc[top_50_currencies_df['volume_market_cap_ratio'] > 1, 'volumefrom'] = 1e-10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92874877-b599-4bb5-94da-b6114f01d167",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def fill_zero_volumefrom(group):\n",
    "    \n",
    "    group = group.copy()  # 复制数据以防止修改原始数据\n",
    "\n",
    "    zero_volume_indices = group[(group['volumefrom'] == 0)|(group['volumefrom'] == 1e-10)].index\n",
    "    zero_dates = group.loc[zero_volume_indices].index.get_level_values('time')\n",
    "    \n",
    "    if len(zero_volume_indices) == 0:\n",
    "        return group\n",
    "    currency = group.loc[zero_volume_indices].index.get_level_values('Currency')[0]  # Assuming all rows in the group have the same currency\n",
    "    \n",
    "    # 如果有连续为0的日期，找出这些连续的序列\n",
    "    continuous_zero_sequences = []\n",
    "    current_sequence = []\n",
    "    \n",
    "    for i, current_date in enumerate(zero_dates):\n",
    "        if i == 0 or (zero_dates[i] - zero_dates[i-1]).days == 1:\n",
    "            current_sequence.append(current_date)\n",
    "        else:\n",
    "            continuous_zero_sequences.append(current_sequence)\n",
    "            current_sequence = [current_date]\n",
    "            \n",
    "    if current_sequence:\n",
    "        continuous_zero_sequences.append(current_sequence)\n",
    "    \n",
    "    # 使用开始连续为0的前一天，和不在连续为0的第一天来填充\n",
    "    for sequence in continuous_zero_sequences:\n",
    "        if not sequence:\n",
    "            continue  # Skip empty sequences\n",
    "\n",
    "        start_date = sequence[0] - pd.Timedelta(days=1)\n",
    "        end_date = sequence[-1] + pd.Timedelta(days=1)\n",
    "        \n",
    "        # Check if start_date and end_date actually exist in the index\n",
    "        if (currency, start_date) in group.index and (currency, end_date) in group.index:\n",
    "            start_close = group.loc[(currency, start_date), 'close']\n",
    "            end_open = group.loc[(currency, end_date), 'open']\n",
    "\n",
    "            for zero_date in sequence:\n",
    "                group.loc[(currency, zero_date), 'open'] = start_close\n",
    "                group.loc[(currency, zero_date), 'close'] = end_open\n",
    "                group.loc[(currency, zero_date), 'high'] = max(start_close, end_open)\n",
    "                group.loc[(currency, zero_date), 'low'] = min(start_close, end_open)\n",
    "        else:\n",
    "            print(f\"Skipping sequence from {start_date} to {end_date} as they are not in the index.\")\n",
    "    \n",
    "    return group\n",
    "\n",
    "# 使用groupby和apply方法\n",
    "top_50_currencies_df = top_50_currencies_df.groupby('Currency', group_keys=False).apply(fill_zero_volumefrom)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d9fd783-e448-4c26-9e60-17064578d9af",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "top_50_currencies_df.loc[top_50_currencies_df['volumefrom'] == 1e-10, 'volumefrom'] = np.nan\n",
    "top_50_currencies_df.loc[top_50_currencies_df['volumeto'] == 1e-10, 'volumeto'] = np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5a0da37-fe17-4b43-a7a4-e4f78504c61f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def interpolate_zeros(series):\n",
    "    # Find the last valid index which is not 0 and not NaN\n",
    "    last_valid_index = series[series != 0].last_valid_index()\n",
    "    first_valid_index = series[series != 0].first_valid_index()\n",
    "    \n",
    "    # If there's no such index, return the series as is\n",
    "    if last_valid_index is None:\n",
    "        return series\n",
    "\n",
    "    # Get the integer location of this index\n",
    "    last_index = series.index.get_loc(last_valid_index) + 1\n",
    "    first_index = series.index.get_loc(first_valid_index)\n",
    "    \n",
    "    # Create a new series limited to this index\n",
    "    limited_series = series.iloc[first_index:last_index]\n",
    "\n",
    "    # Interpolate the new series\n",
    "    interpolated_series = limited_series.interpolate()\n",
    "    interpolated_series.fillna(method='bfill', inplace=True)\n",
    "    \n",
    "    # Update the original series\n",
    "    series.iloc[first_index:last_index] = interpolated_series\n",
    "\n",
    "    return series\n",
    "\n",
    "# 对每个货币和每一列进行处理\n",
    "for col in ['active_addresses', 'new_addresses', 'volumefrom', 'volumeto']:\n",
    "    combined_df[col] = combined_df.groupby('Currency', group_keys=False)[col].apply(interpolate_zeros)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7463bf6f-2dcc-434c-b2d4-76c298735095",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_50_currencies_df['volumeTotal'] = top_50_currencies_df['close'] * top_50_currencies_df['volumefrom'] + top_50_currencies_df['volumeto']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d03992fd-d073-40b4-94f1-d74b4e67755e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "num_currencies = top_50_currencies_df.index.get_level_values('Currency').nunique()\n",
    "print(num_currencies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4070807-10bb-4cb2-89c1-36452e515644",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "top_50_currencies_df.drop('conversionType', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a6e4765-329d-4343-92c2-e2ed02e6fe76",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "top_50_currencies_df.drop('conversionSymbol', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "967b8c53-d6b0-4588-801d-74301aed3e66",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def handle_outliers(group1):\n",
    "    group = group1.copy()\n",
    "    non_nan_ind = ~np.any(np.isnan(group), axis=1)\n",
    "    group = group[non_nan_ind]\n",
    "    \n",
    "    for column in ['volumefrom', 'volumeto']:\n",
    "        index = group[group[column] != 0].index\n",
    "        group2 = group.loc[index].copy()  # 创建一个副本以避免SettingWithCopyWarning\n",
    "        Q1 = group2[column].quantile(0.005)\n",
    "        Q3 = group2[column].quantile(0.995)\n",
    "        \n",
    "        lower_bound = Q1\n",
    "        upper_bound = Q3\n",
    "        \n",
    "        group2[column] = np.where((group2[column] < lower_bound), lower_bound, group2[column])\n",
    "        group2[column] = np.where((group2[column] > upper_bound), upper_bound, group2[column])\n",
    "        \n",
    "        group.loc[index, column] = group2[column]  # 使用.loc一次设置行和列\n",
    "    \n",
    "    group1.loc[non_nan_ind] = group\n",
    "    return group1\n",
    "\n",
    "top_50_currencies_df = top_50_currencies_df.groupby(level=0, group_keys=False).apply(handle_outliers)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0df10726-a5d6-48f1-a7e3-254eeef7dfb9",
   "metadata": {},
   "source": [
    "-----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66555661-4029-4463-93de-48296e07f0f6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def calculate_returns(group1):\n",
    "    group = group1.copy()  # 复制数据以防止修改原始数据\n",
    "    \n",
    "    # 查找首个非NaN和非零的'close'值的索引\n",
    "    start_index = group.loc[(~group['close'].isna()) & (group['close'] != 0), 'close'].index[0]\n",
    "    \n",
    "    # 根据start_index裁剪数据框架\n",
    "    group = group.loc[start_index:]\n",
    "    \n",
    "    group['return'] = group['close'].pct_change()\n",
    "    \n",
    "    # Replace inf with NaN\n",
    "    group['return'].replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "    \n",
    "    # Back fill NaN values\n",
    "    group['return'].fillna(method='bfill', inplace=True)\n",
    "    \n",
    "    # Update the original df with the calculations from valid_df\n",
    "    group1.loc[start_index:, 'return'] = group['return']\n",
    "    \n",
    "    return group1\n",
    "\n",
    "top_50_currencies_df = top_50_currencies_df.groupby(level=0, group_keys=False).apply(calculate_returns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0864532e-1c70-43df-834a-f5ad454a4fd2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def handle_outliers(group1):\n",
    "    group = group1.copy()\n",
    "    non_nan_ind = ~np.any(np.isnan(group), axis=1)\n",
    "    group = group[non_nan_ind]\n",
    "    \n",
    "    for column in ['return']:\n",
    "        index = group[group[column] != 0].index\n",
    "        group2 = group.loc[index].copy()  # 创建一个副本以避免SettingWithCopyWarning\n",
    "        Q3 = group2[column].quantile(0.99)\n",
    "\n",
    "        upper_bound = Q3\n",
    "        \n",
    "        group2[column] = np.where((group2[column] > upper_bound), upper_bound, group2[column])\n",
    "        \n",
    "        group.loc[index, column] = group2[column]  # 使用.loc一次设置行和列\n",
    "    \n",
    "    group1.loc[non_nan_ind] = group\n",
    "    \n",
    "    return group1\n",
    "\n",
    "top_50_currencies_df = top_50_currencies_df.groupby(level=0, group_keys=False).apply(handle_outliers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e09ceed0-4b3b-4ca1-aa85-3bed04646f1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_excess_returns(group1):\n",
    "    \n",
    "    group = group1.copy()  # 复制数据以防止修改原始数据\n",
    "    \n",
    "    # 查找首个非NaN和非零的'close'值的索引\n",
    "    start_time = group.loc[(~group['close'].isna()) & (group['close'] != 0), 'close'].index.get_level_values('time')[0]\n",
    "    \n",
    "    # 根据start_time裁剪数据框架\n",
    "    group = group.loc[pd.IndexSlice[:, start_time:], :]\n",
    "    \n",
    "    # 从rf_df中取出相应的日回报数据\n",
    "    group_rf = rf_df.loc[start_time:]\n",
    "    \n",
    "    # 确保 group 和 group_rf 的索引对齐\n",
    "    group_rf = group_rf.reindex(group.index.get_level_values('time'))\n",
    "    \n",
    "    group['excess_return'] = group['return'] - group_rf['daily_return']\n",
    "    \n",
    "    # Update the original df with the calculations from valid_df\n",
    "    group1.loc[pd.IndexSlice[:, start_time:], 'excess_return'] = group['excess_return']\n",
    "    \n",
    "    return group1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d82c085-1411-4cf4-a636-97a4f384b964",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "top_50_currencies_df = top_50_currencies_df.groupby(level='Currency', group_keys=False).apply(calculate_excess_returns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eaed424-ae7e-40fa-9547-0fb1c0dacf81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Then, calculate the weighted return for each asset\n",
    "top_50_currencies_df['weighted_return'] = top_50_currencies_df['return'] * top_50_currencies_df['weight']\n",
    "\n",
    "# Calculate the return of the market portfolio for each day\n",
    "market_return = top_50_currencies_df.groupby(level='time', group_keys=False)['weighted_return'].sum()\n",
    "\n",
    "# Calculate the excess return, i.e., the market return minus the risk-free rate\n",
    "excess_mkt_return = market_return - rf_df['daily_return']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f0b8da9-71ad-4275-834d-b534303375e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 创建保存CSV文件的文件夹，如果不存在的话\n",
    "folder_name = \"variable_csvs\"\n",
    "if not os.path.exists(folder_name):\n",
    "    os.makedirs(folder_name)\n",
    "\n",
    "# 按照Currency分组并保存每个组为一个CSV文件\n",
    "for name, group in top_50_currencies_df.groupby(level='Currency', group_keys=False):\n",
    "    file_path = os.path.join(folder_name, f\"{name}.csv\")\n",
    "    group.to_csv(file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7555b27b-ef9e-43b3-b93b-3cdacb9d8acc",
   "metadata": {},
   "source": [
    "------\n",
    "### Construct characteristics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "336ee832-bc86-445d-8ff4-53b7f8aedd7e",
   "metadata": {},
   "source": [
    "#### 1. new_add"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ae10181-50df-4fd7-98ee-933f8df851f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 创建一个新的DataFrame，只包含 volumeTotal 列\n",
    "depend_variable_df = top_50_currencies_df[['new_addresses']].copy()\n",
    "\n",
    "# 确保新的DataFrame有 double index\n",
    "depend_variable_df = depend_variable_df.reset_index()\n",
    "depend_variable_df.set_index(['Currency', 'time'], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "451c8b5a-2a50-4229-807f-c0358f8ab2c1",
   "metadata": {},
   "source": [
    "#### 2. active_add"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f967220e-a815-4714-9d64-4516e7573cae",
   "metadata": {},
   "outputs": [],
   "source": [
    "depend_variable_df = depend_variable_df.join(top_50_currencies_df[['active_addresses']])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f44d6df-a5d9-40cc-baf0-29f88b0d3598",
   "metadata": {},
   "source": [
    "#### 3. bm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e523423-e1d6-448d-82d0-c0a5256672ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_network_to_market_value(df1):\n",
    "    df = df1.copy()\n",
    "    \n",
    "    # Find the first non-NaN new_addresses value\n",
    "    start_index = df.loc[(~df['close'].isna()) & (df['close'] != 0), 'close'].index[0]\n",
    "    \n",
    "    # Slice the dataframe from the start_index\n",
    "    valid_df = df.loc[start_index:]\n",
    "    \n",
    "    # Calculate cumulative unique addresses\n",
    "    valid_df['cumulative_unique_addresses'] = valid_df['new_addresses'].cumsum()\n",
    "    \n",
    "    # Calculate network-to-market value\n",
    "    valid_df['network_to_market_value'] = valid_df['cumulative_unique_addresses'] / (valid_df['current_supply']*valid_df['close'])\n",
    "\n",
    "    # Update the original dataframe with the calculated values\n",
    "    df1.loc[start_index:, 'cumulative_unique_addresses'] = valid_df['cumulative_unique_addresses']\n",
    "    df1.loc[start_index:, 'network_to_market_value'] = valid_df['network_to_market_value']\n",
    "\n",
    "    return df1\n",
    "\n",
    "# Apply the function to each group of cryptocurrencies\n",
    "top_50_currencies_df = top_50_currencies_df.groupby(level='Currency', group_keys=False).apply(calculate_network_to_market_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23c63f2e-4518-49ba-afca-b38f7f4d47f7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "depend_variable_df = depend_variable_df.join(top_50_currencies_df[['network_to_market_value']])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fce4fc3-91b1-4d49-beed-059736283381",
   "metadata": {},
   "source": [
    "#### 4. volume"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad2a2788-5eaf-4ced-84ce-d756d0721f7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "depend_variable_df = depend_variable_df.join(top_50_currencies_df[['volumeTotal']])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a1731fe-a49e-4492-b8ba-cf0ac7e21b40",
   "metadata": {},
   "source": [
    "#### 5. size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e77cbba-70ca-4e62-b26c-bff755013f30",
   "metadata": {},
   "outputs": [],
   "source": [
    "depend_variable_df = depend_variable_df.join(top_50_currencies_df[['mark_cap']])\n",
    "depend_variable_df.rename(columns={'mark_cap': 'size'}, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a6f4d1f-6a86-45d0-80c8-927bb4067273",
   "metadata": {},
   "source": [
    "#### 6. rvol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90769c86-b506-4f94-895b-4678e117f225",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_volatility(df1, N=2):\n",
    "    df = df1.copy()\n",
    "    \n",
    "    # Find the index of the first non-NaN and non-zero 'close' value\n",
    "    first_valid_idx = df.loc[(~df['close'].isna()) & (df['close'] != 0), 'close'].index[0]\n",
    "    valid_df = df.loc[first_valid_idx:]\n",
    "    \n",
    "    # Compute the desired metrics only on valid data\n",
    "    valid_df['log_co'] = np.log(valid_df['close'] / valid_df['open'])\n",
    "    valid_df['log_oc'] = np.log(valid_df['open'] / valid_df['close'].shift(1))\n",
    "    valid_df['log_ho'] = np.log(valid_df['high'] / valid_df['open'])\n",
    "    valid_df['log_lo'] = np.log(valid_df['low'] / valid_df['open'])\n",
    "    valid_df['log_hc'] = np.log(valid_df['high'] / valid_df['close'])\n",
    "    valid_df['log_lc'] = np.log(valid_df['low'] / valid_df['close'])\n",
    "\n",
    "    # Applying the modifications\n",
    "    valid_df['log_ho'] = valid_df['log_ho'].apply(lambda x: 0 if x < 0 else x)\n",
    "    valid_df['log_lo'] = valid_df['log_lo'].apply(lambda x: 0 if x > 0 else x)\n",
    "\n",
    "    # open to close volatility\n",
    "    valid_df['vol_oc'] = valid_df['log_oc'].rolling(N).std()\n",
    "\n",
    "    # overnight volatility\n",
    "    valid_df['vol_co'] = valid_df['log_co'].rolling(N).std()\n",
    "\n",
    "    # Rogers-Satchell volatility\n",
    "    valid_df['vol_rs'] = np.sqrt(valid_df[['log_ho', 'log_lo', 'log_hc', 'log_lc']].apply(lambda x: x[0]*x[2] + x[1]*x[3], axis=1).rolling(N).mean())\n",
    "\n",
    "    # Yang-Zhang\n",
    "    k = 0.34 / (1.34 + (N+1) / (N-1))\n",
    "    valid_df['vol_yz'] = np.sqrt(valid_df['vol_co']**2 + k * valid_df['vol_oc']**2 + (1-k) * valid_df['vol_rs']**2)\n",
    "\n",
    "    # Update the original df with the calculations from valid_df\n",
    "    df1.loc[first_valid_idx:, 'vol_yz'] = valid_df['vol_yz']\n",
    "\n",
    "    return df1\n",
    "\n",
    "N = 2\n",
    "volatility_df = top_50_currencies_df.groupby(level='Currency', group_keys=False).apply(compute_volatility, N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7427cf5a-3692-42f9-976e-372263649725",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 在 volatility_df 中选择 vol_yz 列，并创建一个新的 DataFrame\n",
    "vol_yz = volatility_df[['vol_yz']]\n",
    "\n",
    "# 将 vol_yz_df 添加到 depend_variable_df 中\n",
    "depend_variable_df = depend_variable_df.join(vol_yz)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9783d44e-3680-4fbc-af1d-26f8384e646f",
   "metadata": {},
   "source": [
    "#### 7. bid-ask "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec0479e8-cd36-471a-9093-4f2e0c50e7e9",
   "metadata": {},
   "source": [
    "##### 7.1 Abdi and Ranaldo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91740ad6-41e3-42d0-8100-237831d1f3b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_spread(df1):\n",
    "    # 创建一个新的数据框架的副本以防止对原始数据进行更改\n",
    "    df = df1.copy()\n",
    "    \n",
    "    # 查找首个非NaN和非零的'close'值的索引\n",
    "    start_index = df.loc[(~df['close'].isna()) & (df['close'] != 0), 'close'].index[0]\n",
    "\n",
    "    # 根据start_index裁剪数据框架\n",
    "    valid_df = df.loc[start_index:]\n",
    "\n",
    "    valid_df['log_close'] = np.log(valid_df['close'])  # 计算收盘价格的对数\n",
    "    valid_df['log_mid'] = np.log((valid_df['high'] + valid_df['low']) / 2)  # 计算中间价格的对数\n",
    "\n",
    "    # 使用滞后的mid和close来计算spread\n",
    "    valid_df['log_mid_lag'] = valid_df['log_mid'].shift(1)\n",
    "\n",
    "    # 计算两天的log price差值的乘积\n",
    "    valid_df['prod_diff'] = (valid_df['log_close'] - valid_df['log_mid']) * (valid_df['log_close'] - valid_df['log_mid_lag'])\n",
    "\n",
    "    # 检查prod_diff是否大于0，如果不是则设为0\n",
    "    valid_df['prod_diff'] = valid_df['prod_diff'].apply(lambda x: max(4*x, 0))\n",
    "\n",
    "    # Update the original df with the calculations from valid_df\n",
    "    df1.loc[start_index:, 'bid-ask_Abdi'] = np.sqrt(valid_df['prod_diff'])\n",
    "\n",
    "    return df1\n",
    "\n",
    "bidask_df1 = top_50_currencies_df.groupby(level='Currency', group_keys=False).apply(calculate_spread)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31e2d924-68a8-4923-8573-a622ae632e62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 将 vol_yz_df 添加到 depend_variable_df 中\n",
    "depend_variable_df = depend_variable_df.join(bidask_df1[['bid-ask_Abdi']])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcc34872-a320-4077-8416-0b7ea5b11ded",
   "metadata": {},
   "source": [
    "##### 7.2 Corwin and Schultz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d183498-5f57-4811-a244-5c3b95a681da",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_spread(df1):\n",
    "    # 创建一个新的数据框架的副本以防止对原始数据进行更改\n",
    "    df = df1.copy()\n",
    "    \n",
    "    # 查找首个非NaN和非零的'close'值的索引\n",
    "    start_index = df.loc[(~df['close'].isna()) & (df['close'] != 0), 'close'].index[0]\n",
    "    \n",
    "    # 根据start_index裁剪数据框架\n",
    "    valid_df = df.loc[start_index:]\n",
    "    \n",
    "    # calculate two day high and low\n",
    "    valid_df['H_t_t+1'] = valid_df['high'].rolling(2).max()\n",
    "    valid_df['L_t_t+1'] = valid_df['low'].rolling(2).min()\n",
    "\n",
    "    # calculate beta and gamma\n",
    "    valid_df['beta'] = ((np.log(valid_df['high'] / valid_df['low']))**2).rolling(2).sum()\n",
    "    valid_df['gamma'] = np.log(valid_df['H_t_t+1'] / valid_df['L_t_t+1'])**2\n",
    "\n",
    "    # calculate alpha and Spread\n",
    "    valid_df['alpha'] = (np.sqrt(2 * valid_df['beta']) - np.sqrt(valid_df['beta'])) / (3 - 2 * np.sqrt(2)) - np.sqrt(valid_df['gamma'] / (3 - 2 * np.sqrt(2)))\n",
    "    valid_df['Spread'] = (2 * (np.exp(valid_df['alpha']) - 1)) / (1 + np.exp(valid_df['alpha']))\n",
    "\n",
    "    # 检查prod_diff是否大于0，如果不是则设为0\n",
    "    valid_df['Spread'] = valid_df['Spread'].apply(lambda x: max(x, 0))\n",
    "    \n",
    "    # Update the original df with the calculations from valid_df\n",
    "    df1.loc[start_index:, 'bid-ask_Corwin'] = valid_df['Spread']\n",
    "    \n",
    "    return df1\n",
    "\n",
    "bidask_df2 = top_50_currencies_df.groupby(level='Currency', group_keys=False).apply(calculate_spread)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67c0b7e7-1428-48de-9cc4-da0b2479a5c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 将 vol_yz_df 添加到 depend_variable_df 中\n",
    "depend_variable_df = depend_variable_df.join(bidask_df2[['bid-ask_Corwin']])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4ff3526-038b-44f3-a9ac-4bea4314498f",
   "metadata": {},
   "source": [
    "##### 8.1 Zeros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45babfc2-1019-4943-b945-66c8bb4c1639",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_illiq(df1):\n",
    "    # 创建一个新的数据框架的副本以防止对原始数据进行更改\n",
    "    df = df1.copy()\n",
    "    \n",
    "    start_index = df.loc[(~df['close'].isna()) & (df['close'] != 0), 'close'].index[0]\n",
    "    \n",
    "    # 根据start_index裁剪数据框架\n",
    "    valid_df = df.loc[start_index:]\n",
    "    \n",
    "    # 计算过去30天内'return'为0的天数\n",
    "    valid_df['Zeros'] = valid_df['return'].rolling(window=30).apply(lambda x: (x == 0).sum())/30\n",
    "    \n",
    "    # Update the original df with the calculations from valid_df\n",
    "    df1.loc[start_index:, 'Zeros'] = valid_df['Zeros']\n",
    "    \n",
    "    return df1\n",
    "\n",
    "# 使用你的原始DataFrame\n",
    "illiq_zeros = top_50_currencies_df.groupby(level='Currency', group_keys=False).apply(calculate_illiq)\n",
    "\n",
    "depend_variable_df = depend_variable_df.join(illiq_zeros[['Zeros']])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4741b8a-a39c-49cc-be87-58048831958f",
   "metadata": {},
   "source": [
    "#### 8. illiq "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3bde07b-1630-4c08-856e-b2a9f1a2f223",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_illiq(df1):\n",
    "    df = df1.copy()  # 复制数据以防止修改原始数据\n",
    "    \n",
    "    # 查找首个非NaN和非零的'close'值的索引\n",
    "    start_index = df.loc[(~df['close'].isna()) & (df['close'] != 0), 'close'].index[0]\n",
    "    \n",
    "    # 根据start_index裁剪数据框架\n",
    "    df = df.loc[start_index:]\n",
    "    \n",
    "    df['intraday_return'] = df['close'] / df['open'] - 1  # 计算日内收益\n",
    "    df['illiq'] = abs(df['intraday_return']) / df['volumeTotal']  # 计算Illiquidity Ratio\n",
    "    if np.any(df['volumeTotal']) == 0:\n",
    "        print(df)\n",
    "    \n",
    "    # Update the original df with the calculations from valid_df\n",
    "    df1.loc[start_index:, 'illiq'] = df['illiq']\n",
    "    \n",
    "    return df1\n",
    "\n",
    "illiq_df = top_50_currencies_df.groupby(level='Currency', group_keys=False).apply(calculate_illiq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47c5dafc-9d9a-4f98-9635-e2dc44c52505",
   "metadata": {},
   "outputs": [],
   "source": [
    "illiq_df = illiq_df[['illiq']]\n",
    "\n",
    "depend_variable_df = depend_variable_df.join(illiq_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d70cff9c-8dfc-408e-b34d-f96464879ab4",
   "metadata": {},
   "outputs": [],
   "source": [
    "depend_variable_df['illiq'] = depend_variable_df.groupby('Currency')['illiq'].transform(lambda x: x.replace(np.inf, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c31f824-756b-40ac-9c4f-32a006876188",
   "metadata": {},
   "source": [
    "#### 9. capm  beta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42fefe97-69d9-42fe-a6e7-d3f55ac9f579",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 将 Series 转换为 DataFrame，并将新列命名为 'excess_return'\n",
    "excess_mkt_return_df = excess_mkt_return.to_frame(name='excess_return')\n",
    "\n",
    "# 确保索引是日期类型\n",
    "excess_mkt_return_df.index = pd.to_datetime(excess_mkt_return_df.index)\n",
    "\n",
    "excess_mkt_return_df['lagged_excess_return'] = excess_mkt_return.shift(1)\n",
    "\n",
    "excess_mkt_return_df['lagged_excess_return'].fillna(method='bfill', inplace=True)\n",
    "\n",
    "# 删除可能的NaN值\n",
    "#excess_return.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "befb7a66-13ea-4435-9628-920ac386d02e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#top_50_currencies_df[top_50_currencies_df['close']==0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07fc8ae2-12df-490d-86fa-ed6650d9d907",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def calculate_beta(group1):\n",
    "    \n",
    "    group = group1.copy()  # 复制数据以防止修改原始数据\n",
    "    \n",
    "    # 查找首个非NaN和非零的'close'值的索引\n",
    "    start_index = group.loc[(~group['close'].isna()) & (group['close'] != 0), 'close'].index[0]\n",
    "    end_index = group.loc[(~group['close'].isna()), 'close'].index[-1]\n",
    "    \n",
    "    # 根据start_index裁剪数据框架\n",
    "    group = group.loc[start_index:end_index]\n",
    "    \n",
    "    if len(group)<30:\n",
    "        group1.loc[start_index:end_index, 'beta'] = np.nan\n",
    "        group1.loc[start_index:end_index, 'alpha'] = np.nan\n",
    "        group1.loc[start_index:end_index, 'ivol'] = np.nan\n",
    "        print(group1)\n",
    "        return group1\n",
    "        \n",
    "    \n",
    "    # 使用滚动窗口计算Beta\n",
    "    for i in range(30, len(group)):\n",
    "        window = group[i-30:i]  # 30-day rolling window\n",
    "        window2 = excess_mkt_return_df.loc[window.index.get_level_values('time')]  # ensure the index of window2 is the same as window\n",
    "\n",
    "        Y = window['return']  # dependent variable\n",
    "        Y.index = Y.index.get_level_values('time')  # Only keep 'time' level of the index\n",
    "        \n",
    "        X = window2[['excess_return', 'lagged_excess_return']]  # independent variables\n",
    "        X = sm.add_constant(X)  # add a constant term to the independent variables\n",
    "        \n",
    "        '''\n",
    "        # Check for NaNs and infs in X and print them\n",
    "        if X.isnull().values.any() or np.isinf(X.values).any():\n",
    "            print(f\"NaNs or infs detected at index {i} for Currency {group.index.get_level_values('Currency')[0]}:\")\n",
    "            print(X)\n",
    "        '''\n",
    "        \n",
    "        model = sm.OLS(Y, X)\n",
    "        results = model.fit()\n",
    "        \n",
    "        group.loc[group.index[i], 'beta'] = results.params['excess_return']+results.params['lagged_excess_return']  # get the coefficient of market excess return\n",
    "        group.loc[group.index[i], 'alpha'] = results.params['const']\n",
    "        group.loc[group.index[i], 'ivol'] = np.std(results.resid)  # get the standard deviation of residuals (ivol)\n",
    "        \n",
    "    idx_position_in_group1 = group1.index.get_loc(group.index[30])\n",
    "\n",
    "    # Update the original df with the calculations from valid_df\n",
    "    group1.loc[start_index:end_index, 'beta'] = group['beta']\n",
    "    group1.loc[start_index:end_index, 'alpha'] = group['alpha']\n",
    "    group1.loc[start_index:end_index, 'ivol'] = group['ivol']\n",
    "        \n",
    "    return group1\n",
    "\n",
    "# 对每个货币计算Beta\n",
    "capm_df = top_50_currencies_df.groupby(level='Currency', group_keys=False).apply(calculate_beta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d860e3ce-d81c-42fa-a2db-79e00d1f331d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "beta_df = capm_df[['beta']]\n",
    "\n",
    "depend_variable_df = depend_variable_df.join(beta_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e515b0ee-23e9-4fb6-826c-f27a5f7704cb",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### 10. capm  alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26f07ade-a310-427c-b16b-c80259c5b32a",
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha_df = capm_df[['alpha']]\n",
    "\n",
    "depend_variable_df = depend_variable_df.join(alpha_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0cdc61b-b7fe-444a-b8ae-d9038febe1c9",
   "metadata": {},
   "source": [
    "#### 11. ivol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a3f113c-2816-4ae0-a867-270fbda43f3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "ivol_df = capm_df[['ivol']]\n",
    "\n",
    "depend_variable_df = depend_variable_df.join(ivol_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a92ae988-5958-4654-8268-b520d11383c6",
   "metadata": {},
   "source": [
    "#### 12. turnover"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d63fb79-720f-4790-85bd-26b81ecce43f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 创建一个函数来计算每个组的换手率\n",
    "def calculate_turnover(df1):\n",
    "    df = df1.copy()\n",
    "    \n",
    "    # Find the first non-NaN new_addresses value\n",
    "    start_index = df.loc[(~df['close'].isna()) & (df['close'] != 0), 'close'].index[0]\n",
    "    \n",
    "    # Slice the dataframe from the start_index\n",
    "    valid_df = df.loc[start_index:]\n",
    "    \n",
    "    valid_df['past'] = valid_df['volumeTotal'].shift(1)\n",
    "    valid_df['past'].fillna(method='bfill', inplace=True)\n",
    "    \n",
    "    valid_df['turnover'] = valid_df['past'] / (valid_df['current_supply']*valid_df['close'])\n",
    "    \n",
    "    # Update the original df with the calculations from valid_df\n",
    "    df1.loc[start_index:, 'turnover'] = valid_df['turnover']\n",
    "    \n",
    "    return df1\n",
    "\n",
    "# 对每个货币计算换手率\n",
    "top_50_currencies_df = top_50_currencies_df.groupby(level='Currency', group_keys=False).apply(calculate_turnover)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7480db44-9203-4291-9e97-b8df98f60766",
   "metadata": {},
   "outputs": [],
   "source": [
    "depend_variable_df = depend_variable_df.join(top_50_currencies_df[['turnover']])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e38341b5-23e1-4fff-baec-3597d822aec4",
   "metadata": {},
   "source": [
    "#### 12.1. Turnover-Adjusted Number of Zero Daily Trading Volume"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a9307c7-3012-43c1-9000-d7e1891d8f65",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_LM(turnover_sum):\n",
    "    if turnover_sum == 0:\n",
    "        return 1\n",
    "    else:\n",
    "        return 1 / (turnover_sum * 4800000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26f572ad-c3be-40bd-8222-00ba2f9cf314",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 创建一个函数来计算每个组的换手率\n",
    "def calculate_turnover_adjusted(df1):\n",
    "    df = df1.copy()\n",
    "    \n",
    "    # Find the first non-NaN new_addresses value\n",
    "    start_index = df.loc[(~df['close'].isna()) & (df['close'] != 0), 'close'].index[0]\n",
    "    \n",
    "    # Slice the dataframe from the start_index\n",
    "    valid_df = df.loc[start_index:]\n",
    "    \n",
    "    valid_df['zeros'] = valid_df['volumeTotal'].rolling(window=30).apply(lambda x: (x == 0).sum())\n",
    "    \n",
    "    valid_df['turnover_30day_sum'] = valid_df['turnover'].rolling(window=30).sum()\n",
    "    valid_df['LM'] = valid_df['turnover_30day_sum'].apply(calculate_LM)\n",
    "    \n",
    "    valid_df['turnover_adjusted'] = (valid_df['zeros'] + valid_df['LM'])*30/30\n",
    "    \n",
    "    # Update the original df with the calculations from valid_df\n",
    "    df1.loc[start_index:, 'turnover_adjusted'] = valid_df['turnover_adjusted']\n",
    "    \n",
    "    return df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d5c864e-385c-4b5c-9d52-fc33e8dc170a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 对每个货币计算换手率\n",
    "top_50_currencies_df = top_50_currencies_df.groupby(level='Currency', group_keys=False).apply(calculate_turnover_adjusted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68325344-1875-49b9-b227-3494bfe863f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "depend_variable_df = depend_variable_df.join(top_50_currencies_df[['turnover_adjusted']])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ebb30e9-7510-4c39-b934-70c036fbfb6b",
   "metadata": {},
   "source": [
    "#### 13. dto    180 days Q!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b26a6f6e-cca9-4ef5-87ab-3b8acad11f8b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 计算每天的市场换手率\n",
    "top_50_currencies_df['weighted_turnover'] = top_50_currencies_df['turnover'] * top_50_currencies_df['weight']\n",
    "daily_market_turnover = top_50_currencies_df.groupby(level='time', group_keys=False)['weighted_turnover'].sum() \n",
    "daily_market_turnover.name = 'market_turnover'\n",
    "\n",
    "# 合并每天的市场换手率到原始数据框中\n",
    "top_50_currencies_df = top_50_currencies_df.join(daily_market_turnover)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5941768-2b48-42fd-8221-e6aae44c9e0c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 计算去趋势化的换手率\n",
    "def de_trend_turnover(group1):\n",
    "    group = group1.copy()  # 复制数据以防止修改原始数据\n",
    "    \n",
    "    # 查找首个非NaN和非零的'close'值的索引\n",
    "    start_index = group.loc[(~group['close'].isna()) & (group['close'] != 0), 'close'].index[0]\n",
    "    \n",
    "    # 根据start_index裁剪数据框架\n",
    "    group = group.loc[start_index:]\n",
    "    \n",
    "    group['dto'] = group['turnover'] - group['market_turnover']\n",
    "    group['dto'] = group['dto'] - group['dto'].rolling(180).median() \n",
    "    \n",
    "    # Update the original df with the calculations from valid_df\n",
    "    group1.loc[start_index:, 'dto'] = group['dto']\n",
    "    \n",
    "    return group1\n",
    "\n",
    "top_50_currencies_df = top_50_currencies_df.groupby(level='Currency', group_keys=False).apply(de_trend_turnover)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "630da264-d1aa-4f10-93a3-5f1e3b87b6c8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "depend_variable_df = depend_variable_df.join(top_50_currencies_df[['dto']])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa44e0ac-492f-41d4-b55a-4b56182e099b",
   "metadata": {},
   "source": [
    "#### 14. std_to"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aceeb4b-1264-45b6-a2d5-bad3520f2e46",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_50_currencies_df =  top_50_currencies_df.join(depend_variable_df[['alpha']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be2c53e8-17f3-4810-8581-92e3d13b9a56",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def calculate_std_residuals(group1):\n",
    "    \n",
    "    group = group1.copy()  # 复制数据以防止修改原始数据\n",
    "    \n",
    "    # 根据currency_name从depend_variable_df_copy选择相应的条目\n",
    "    start_index = group.loc[(~group['turnover'].isna()) & (group['turnover'] != 0), 'turnover'].index[0]\n",
    "    end_index = group.loc[(~group['turnover'].isna()) & (group['turnover'] != 0), 'turnover'].index[-1]\n",
    "    \n",
    "    # 根据start_index裁剪数据框架\n",
    "    group = group.loc[start_index:end_index]\n",
    "    \n",
    "    if len(group)<30:\n",
    "        group1['std_to'] = np.nan\n",
    "        print(group1)\n",
    "        return group1\n",
    "\n",
    "    for i in range(30, len(group)):\n",
    "        window = group.iloc[i-30:i]  # 30-day rolling window\n",
    "        Y = window['turnover']\n",
    "        X = np.ones((len(window), 1))\n",
    "\n",
    "        # 检查X中的NaN数量\n",
    "        num_nans = Y.isna().sum()\n",
    "\n",
    "        if num_nans == 0:\n",
    "            model = sm.OLS(Y, X)\n",
    "            results = model.fit()\n",
    "            group.loc[group.index[i], 'std_to'] = results.resid.std()\n",
    "            i += 1  # 移动到下一个窗口\n",
    "        else:\n",
    "            i += num_nans  # 跳过包含NaN的窗口\n",
    "\n",
    "    group1.loc[start_index:end_index, 'std_to'] = group['std_to']\n",
    "\n",
    "    return group1\n",
    "\n",
    "# 对每个货币计算std_residuals\n",
    "top_50_currencies_df = top_50_currencies_df.groupby(level='Currency', group_keys=False).apply(calculate_std_residuals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bbbe828-214a-440a-b825-e76f17380cda",
   "metadata": {},
   "outputs": [],
   "source": [
    "depend_variable_df = depend_variable_df.join(top_50_currencies_df[['std_to']])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b896aa89-136c-46a8-ace7-a8ec98b8af5c",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### 15. std_vol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80ead146-9ea1-43de-a81d-647725834de1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def calculate_std_vol(group1):\n",
    "    \n",
    "    group = group1.copy()  # 复制数据以防止修改原始数据\n",
    "    \n",
    "    # 根据currency_name从depend_variable_df_copy选择相应的条目\n",
    "    start_index = group.loc[(~group['volumeTotal'].isna()) & (group['volumeTotal'] != 0), 'volumeTotal'].index[0]\n",
    "    end_index = group.loc[(~group['volumeTotal'].isna()) & (group['volumeTotal'] != 0), 'volumeTotal'].index[-1]\n",
    "    \n",
    "    # 根据start_index裁剪数据框架\n",
    "    group = group.loc[start_index:end_index]\n",
    "    \n",
    "    if len(group)<30:\n",
    "        group1['std_vol'] = np.nan\n",
    "        return gourp1\n",
    "\n",
    "    for i in range(30, len(group)):\n",
    "        window = group.iloc[i-30:i]  # 30-day rolling window\n",
    "        Y = window['volumeTotal']\n",
    "        X = np.ones((len(window), 1))\n",
    "\n",
    "        # 检查X中的NaN数量\n",
    "        num_nans = Y.isna().sum()\n",
    "\n",
    "        if num_nans == 0:\n",
    "            model = sm.OLS(Y, X)\n",
    "            results = model.fit()\n",
    "            group.loc[group.index[i], 'std_vol'] = results.resid.std()\n",
    "            i += 1  # 移动到下一个窗口\n",
    "        else:\n",
    "            i += num_nans  # 跳过包含NaN的窗口\n",
    "\n",
    "    group1.loc[start_index:end_index, 'std_vol'] = group['std_vol']    \n",
    "    group = group1.copy()  # 复制数据以防止修改原始数据\n",
    "    \n",
    "    return group1\n",
    "\n",
    "# 对每个货币计算std_residuals\n",
    "top_50_currencies_df = top_50_currencies_df.groupby(level='Currency', group_keys=False).apply(calculate_std_vol)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e81909b1-db84-4a2f-aef7-86a13563c2dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "depend_variable_df = depend_variable_df.join(top_50_currencies_df[['std_vol']])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee50a527-777a-4f8c-844e-bafaea22dd1e",
   "metadata": {},
   "source": [
    "#### 16. rel_to_high Q!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea6d4435-dee2-49fa-b173-bf9e9a27ce0f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def calculate_closeness_to_high(df1):\n",
    "    \n",
    "    df = df1.copy()\n",
    "    \n",
    "    # Find the first non-NaN new_addresses value\n",
    "    start_index = df.loc[(~df['close'].isna()) & (df['close'] != 0), 'close'].index[0]\n",
    "    end_index = df.loc[(~df['close'].isna()), 'close'].index[-1]\n",
    "    \n",
    "    # Slice the dataframe from the start_index\n",
    "    valid_df = df.loc[start_index:end_index]\n",
    "    \n",
    "    # Create a rolling window of size 90 on 'close' column and compute the max\n",
    "    valid_df['rolling_90_day_high'] = valid_df['close'].rolling(window=90).max()\n",
    "\n",
    "    # Calculate closeness to 90-day high as ratio of closing price to the rolling 90-day high\n",
    "    valid_df['past'] = valid_df['close'].shift(1)\n",
    "    valid_df['past'].fillna(method='bfill', inplace=True)\n",
    "    \n",
    "    valid_df['rel_to_high'] = valid_df['past'] / valid_df['rolling_90_day_high']\n",
    "    \n",
    "    # Update the original df with the calculations from valid_df\n",
    "    df1.loc[start_index:end_index, 'rel_to_high'] = valid_df['rel_to_high']\n",
    "\n",
    "    return df1\n",
    "\n",
    "# Apply the function to your DataFrame\n",
    "top_50_currencies_df = top_50_currencies_df.groupby(level='Currency', group_keys=False).apply(calculate_closeness_to_high)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10b6c616-1574-4c06-8bb7-1f60cff8e03b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "depend_variable_df = depend_variable_df.join(top_50_currencies_df[['rel_to_high']])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "615f69d9-7812-4035-8a08-3271602b3995",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### 17. max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe8b5d76-88a9-4995-9c01-1433759e4fb0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def calculate_max_return(df1):\n",
    "    \n",
    "    df = df1.copy()\n",
    "    \n",
    "    # Find the first non-NaN new_addresses value\n",
    "    start_index = df.loc[(~df['close'].isna()) & (df['close'] != 0), 'close'].index[0]\n",
    "    end_index = df.loc[(~df['close'].isna()), 'close'].index[-1]\n",
    "    \n",
    "    # Slice the dataframe from the start_index\n",
    "    valid_df = df.loc[start_index:end_index]\n",
    "    \n",
    "    # Create a rolling window of size 30 on 'daily_return' column and compute the max\n",
    "    valid_df['max.'] = valid_df['return'].rolling(window=30).max()\n",
    "    \n",
    "    # Update the original df with the calculations from valid_df\n",
    "    df1.loc[start_index:end_index, 'max.'] = valid_df['max.']\n",
    "    \n",
    "    return df1\n",
    "\n",
    "# Apply the function to your DataFrame\n",
    "top_50_currencies_df = top_50_currencies_df.groupby(level='Currency', group_keys=False).apply(calculate_max_return)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a936a4e4-17a5-4d98-877d-459c3f9e7ed0",
   "metadata": {},
   "outputs": [],
   "source": [
    "depend_variable_df = depend_variable_df.join(top_50_currencies_df[['max.']])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d75eef31-f4b1-4145-912b-38d27c4f6dc9",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### 18. vol_shock"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72ed6719-1b84-4bd9-ba85-b6437e0abc04",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def calculate_volume_shock(df1, window):\n",
    "    \n",
    "    df = df1.copy()\n",
    "    \n",
    "    # Find the first non-NaN new_addresses value\n",
    "    start_index = df.loc[(~df['volumeTotal'].isna()) & (df['volumeTotal'] != 0), 'volumeTotal'].index[0]\n",
    "    end_index = df.loc[(~df['volumeTotal'].isna()) & (df['volumeTotal'] != 0), 'volumeTotal'].index[-1]\n",
    "    \n",
    "    # Slice the dataframe from the start_index\n",
    "    valid_df = df.loc[start_index:end_index]\n",
    "    \n",
    "    # Calculate log of volume\n",
    "    valid_df['log_volume'] = np.log(valid_df['volumeTotal']+1)\n",
    "\n",
    "    # Calculate rolling mean and standard deviation\n",
    "    valid_df[f'rolling_mean_{window}'] = valid_df['log_volume'].rolling(window=window).mean()\n",
    "    valid_df[f'rolling_std_{window}'] = valid_df['log_volume'].rolling(window=window).std()\n",
    "\n",
    "    # Calculate log deviation from trend\n",
    "    valid_df[f'log_dev_{window}'] = valid_df['log_volume'] - valid_df[f'rolling_mean_{window}']\n",
    "\n",
    "    # Standardize log deviation\n",
    "    valid_df[f'vol_shock_{window}d'] = valid_df[f'log_dev_{window}'] / valid_df[f'rolling_std_{window}']\n",
    "\n",
    "    # Update the original df with the calculations from valid_df\n",
    "    df1.loc[start_index:end_index, f'vol_shock_{window}d'] = valid_df[f'vol_shock_{window}d']\n",
    "    \n",
    "    return df1\n",
    "\n",
    "# Apply the function to your DataFrame for l=30,60 days\n",
    "top_50_currencies_df = top_50_currencies_df.groupby(level='Currency', group_keys=False).apply(calculate_volume_shock, window=30)\n",
    "top_50_currencies_df = top_50_currencies_df.groupby(level='Currency', group_keys=False).apply(calculate_volume_shock, window=60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ae55d92-8896-43ff-9e6d-75017b013b02",
   "metadata": {},
   "outputs": [],
   "source": [
    "depend_variable_df = depend_variable_df.join(top_50_currencies_df[['vol_shock_60d']])\n",
    "depend_variable_df = depend_variable_df.join(top_50_currencies_df[['vol_shock_30d']])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "431ec81d-8ef5-4493-b2fb-85a80982f67b",
   "metadata": {},
   "source": [
    "#### 19. r2_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7d4ba9a-5230-4d70-80f3-6599d5144ce3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_r2_1(df1):\n",
    "    \n",
    "    df = df1.copy()\n",
    "    \n",
    "    # Find the first non-NaN new_addresses value\n",
    "    start_index = df.loc[(~df['close'].isna()) & (df['close'] != 0), 'close'].index[0]\n",
    "    end_index = df.loc[(~df['close'].isna()) & (df['close'] != 0), 'close'].index[-1]\n",
    "    \n",
    "    # Slice the dataframe from the start_index\n",
    "    valid_df = df.loc[start_index:end_index]\n",
    "    \n",
    "    # Calculate cumulative return from 2 days ago to 1 day ago\n",
    "    valid_df['r2_1'] = valid_df['return'].shift(1).rolling(window=2).apply(lambda x: (1 + x).prod() - 1)\n",
    "    \n",
    "    # Update the original df with the calculations from valid_df\n",
    "    df1.loc[start_index:end_index, 'r2_1'] = valid_df['r2_1']\n",
    "\n",
    "    return df1\n",
    "\n",
    "# Apply the function to your DataFrame\n",
    "top_50_currencies_df = top_50_currencies_df.groupby(level='Currency', group_keys=False).apply(calculate_r2_1)\n",
    "\n",
    "'''\n",
    "# Rank cryptocurrencies into quintiles based on 'r2_1' at each point in time\n",
    "r2_1_df['quintile'] = r2_1_df.groupby(level='time')['r2_1'].transform(\n",
    "    lambda x: pd.qcut(x, 5, labels=False, duplicates='drop'))\n",
    "\n",
    "# Create the short and long portfolios\n",
    "short_portfolio = r2_1_df[r2_1_df['quintile'] == 4]  # The highest past returns\n",
    "long_portfolio = r2_1_df[r2_1_df['quintile'] == 0]  # The lowest past returns\n",
    "\n",
    "\n",
    "# Calculate the zero investment strategy return\n",
    "zero_investment = long_portfolio.groupby(level='time')['r2_1'].mean() - short_portfolio.groupby(level='time')['r2_1'].mean()\n",
    "\n",
    "# Create a DataFrame to hold the results\n",
    "r2_1_factor = pd.DataFrame(zero_investment, columns=['r2_1'])\n",
    "\n",
    "# Set the DataFrame to have multi-index (time, factor)\n",
    "r2_1_factor.index = pd.MultiIndex.from_product([r2_1_factor.index, ['r2_1']], names=['time', 'factor'])\n",
    "'''\n",
    "depend_variable_df = depend_variable_df.join(top_50_currencies_df[['r2_1']])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a688f5f8-8bbc-43f9-8a7d-b049404f9f48",
   "metadata": {},
   "source": [
    "#### 20.rl_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "831fdb05-089c-413f-9bf9-91733e5082df",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_cumulative_return(df1, l_values):\n",
    "    \n",
    "    df = df1.copy()\n",
    "    \n",
    "    # Find the first non-NaN new_addresses value\n",
    "    start_index = df.loc[(~df['close'].isna()) & (df['close'] != 0), 'close'].index[0]\n",
    "    end_index = df.loc[(~df['close'].isna()) & (df['close'] != 0), 'close'].index[-1]\n",
    "    \n",
    "    # Slice the dataframe from the start_index\n",
    "    valid_df = df.loc[start_index:end_index]\n",
    "    \n",
    "    for l in l_values:\n",
    "        valid_df[f'r{l}_2'] = valid_df['return'].shift(2).rolling(l-2).apply(lambda x: (1 + x).prod() - 1)\n",
    "        \n",
    "        # Update the original df with the calculations from valid_df\n",
    "        df1.loc[start_index:end_index, f'r{l}_2'] = valid_df[f'r{l}_2']\n",
    "\n",
    "    return df1\n",
    "\n",
    "# Apply the function to your DataFrame\n",
    "l_values = [7, 13, 22, 31]\n",
    "top_50_currencies_df = top_50_currencies_df.groupby(level='Currency', group_keys=False).apply(calculate_cumulative_return, l_values=l_values)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b184528-a296-4265-a551-46b1104541c2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "depend_variable_df = depend_variable_df.join(top_50_currencies_df[['r7_2']])\n",
    "depend_variable_df = depend_variable_df.join(top_50_currencies_df[['r13_2']])\n",
    "depend_variable_df = depend_variable_df.join(top_50_currencies_df[['r22_2']])\n",
    "depend_variable_df = depend_variable_df.join(top_50_currencies_df[['r31_2']])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4a7a63d-8b2e-4e0b-befe-ce0b42c62b70",
   "metadata": {},
   "source": [
    "#### 21. r30_14"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15a4eeee-cf89-41d0-8a69-9b113195c683",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_intermediate_momentum(df1):\n",
    "    df = df1.copy()\n",
    "    \n",
    "    # Find the first non-NaN new_addresses value\n",
    "    start_index = df.loc[(~df['close'].isna()) & (df['close'] != 0), 'close'].index[0]\n",
    "    end_index = df.loc[(~df['close'].isna()) & (df['close'] != 0), 'close'].index[-1]\n",
    "    \n",
    "    # Slice the dataframe from the start_index\n",
    "    valid_df = df.loc[start_index:end_index]\n",
    "    \n",
    "    # 计算连续收益率，从30天前到14天前\n",
    "    valid_df['r30_14'] = valid_df['return'].shift(14).rolling(16).apply(lambda x: (1 + x).prod() - 1)\n",
    "    \n",
    "    # Update the original df with the calculations from valid_df\n",
    "    df1.loc[start_index:end_index, 'r30_14'] = valid_df['r30_14']\n",
    "\n",
    "    return df1\n",
    "\n",
    "# 将函数应用到数据框中\n",
    "top_50_currencies_df = top_50_currencies_df.groupby(level='Currency', group_keys=False).apply(calculate_intermediate_momentum)\n",
    "\n",
    "depend_variable_df = depend_variable_df.join(top_50_currencies_df[['r30_14']])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45749f3b-1f94-4985-b15c-f183fe5f1c04",
   "metadata": {},
   "source": [
    "#### 21. r180_60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "267e343f-6fba-4d2a-a48e-73da2abd6219",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_long_term_reversal(df1):\n",
    "    df = df1.copy()\n",
    "    \n",
    "    # Find the first non-NaN new_addresses value\n",
    "    start_index = df.loc[(~df['close'].isna()) & (df['close'] != 0), 'close'].index[0]\n",
    "    end_index = df.loc[(~df['close'].isna()) & (df['close'] != 0), 'close'].index[-1]\n",
    "    \n",
    "    # Slice the dataframe from the start_index\n",
    "    valid_df = df.loc[start_index:end_index]\n",
    "    \n",
    "    # 计算连续收益率，从60天前到180天前\n",
    "    valid_df['r180_60'] = valid_df['return'].shift(60).rolling(120).apply(lambda x: (1 + x).prod() - 1)\n",
    "    \n",
    "    # Update the original df with the calculations from valid_df\n",
    "    df1.loc[start_index:end_index, 'r180_60'] = valid_df['r180_60']\n",
    "\n",
    "    return df1\n",
    "\n",
    "# 将函数应用到数据框中\n",
    "top_50_currencies_df = top_50_currencies_df.groupby(level='Currency', group_keys=False).apply(calculate_long_term_reversal)\n",
    "\n",
    "depend_variable_df = depend_variable_df.join(top_50_currencies_df[['r180_60']])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee7e1e56-b257-42c7-8ba5-62a6ef8f563a",
   "metadata": {},
   "source": [
    "#### 22. VaR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f315d055-a96d-454c-932d-f9bcb198379d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def calculate_historical_var(df1):\n",
    "    df = df1.copy()\n",
    "    \n",
    "    # Find the first non-NaN new_addresses value\n",
    "    start_index = df.loc[(~df['close'].isna()) & (df['close'] != 0), 'close'].index[0]\n",
    "    end_index = df.loc[(~df['close'].isna()) & (df['close'] != 0), 'close'].index[-1]\n",
    "    \n",
    "    # Slice the dataframe from the start_index\n",
    "    valid_df = df.loc[start_index:end_index]\n",
    "    \n",
    "    valid_df['VaR_5%'] = valid_df['return'].rolling(window=90).apply(lambda x: np.percentile(x, 5))\n",
    "    \n",
    "    # Update the original df with the calculations from valid_df\n",
    "    df1.loc[start_index:end_index, 'VaR_5%'] = valid_df['VaR_5%']\n",
    "\n",
    "    return df1\n",
    "\n",
    "# Apply the function to the DataFrame\n",
    "top_50_currencies_df = top_50_currencies_df.groupby(level='Currency', group_keys=False).apply(calculate_historical_var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb054325-a863-4351-892f-bb456de66b31",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "depend_variable_df = depend_variable_df.join(top_50_currencies_df[['VaR_5%']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d228f0f0-31af-42a8-82fb-3e3f4f9fa6be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 创建保存CSV文件的文件夹，如果不存在的话\n",
    "folder_name = \"currency_csvs\"\n",
    "if not os.path.exists(folder_name):\n",
    "    os.makedirs(folder_name)\n",
    "\n",
    "# 按照Currency分组并保存每个组为一个CSV文件\n",
    "for name, group in depend_variable_df.groupby(level='Currency', group_keys=False):\n",
    "    file_path = os.path.join(folder_name, f\"{name}.csv\")\n",
    "    group.to_csv(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f13f4d1-73df-4523-b9bf-ac8752c487f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "depend_variable_df.describe().to_csv(\"chara_describe.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "958a4bcf-5ff8-4854-bbb2-6f2fb5cf7788",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "non_nan_ind = ~np.any(np.isnan(depend_variable_df), axis=1)\n",
    "depend_variable_df.loc[non_nan_ind].describe().to_csv(\"chara_describe.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f34fde1-00ff-4fbb-8bad-130fbd1b8f01",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def handle_outliers(group1):\n",
    "    group = group1.copy()\n",
    "    non_nan_ind = ~np.any(np.isnan(group), axis=1)\n",
    "    group = group[non_nan_ind]\n",
    "    \n",
    "    for column in ['illiq']:\n",
    "        index = group[group[column] != 0].index\n",
    "        group2 = group.loc[index].copy()  # 创建一个副本以避免SettingWithCopyWarning\n",
    "        Q3 = group2[column].quantile(0.995)\n",
    "\n",
    "        upper_bound = Q3\n",
    "        \n",
    "        group2[column] = np.where((group2[column] > upper_bound), upper_bound, group2[column])\n",
    "        \n",
    "        group.loc[index, column] = group2[column]  # 使用.loc一次设置行和列\n",
    "    \n",
    "    group1.loc[non_nan_ind] = group\n",
    "    \n",
    "    return group1\n",
    "\n",
    "depend_variable_df = depend_variable_df.groupby(level=0, group_keys=False).apply(handle_outliers)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57bd9c5b-1ebf-4ef9-855a-9b901941c508",
   "metadata": {},
   "source": [
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9556ff5-5bb7-4a1d-ac14-407e51d7a6fd",
   "metadata": {
    "tags": []
   },
   "source": [
    "# monthly returns and excess returns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cfe55f1-8d2b-437a-be1f-e7d32cdb482d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def compute_rolling_30day_return(df_currency1):\n",
    "    \n",
    "    df_currency = df_currency1.copy()\n",
    "\n",
    "    # 重置日期索引\n",
    "    df_currency = df_currency.reset_index(level='Currency', drop=True)\n",
    "    \n",
    "    # 计算每日的累积回报\n",
    "    df_currency['Cumulative Periodic Return'] = (1 + df_currency['return']).cumprod()\n",
    "    \n",
    "    rolling_return = df_currency['Cumulative Periodic Return'].rolling(window=30).apply(lambda x: x[-1]/x[0] - 1)\n",
    "    \n",
    "    # 创建一个新的DataFrame来存储这些回报\n",
    "    df_rolling_return = pd.DataFrame({\n",
    "        'monthly Return': rolling_return\n",
    "    })\n",
    "    \n",
    "    # 只为日期设置索引，不再为货币设置索引\n",
    "    df_rolling_return.index.name = 'Date'\n",
    "    \n",
    "    return df_rolling_return\n",
    "\n",
    "# 应用上述函数到每一个货币\n",
    "all_monthly_returns = top_50_currencies_df.groupby(level='Currency').apply(compute_rolling_30day_return)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a4a6309-b339-42bb-9d0c-c564c08b328b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# exclude first month\n",
    "all_monthly_returns = all_monthly_returns.groupby(level='Currency').apply(lambda x: x.iloc[6:])\n",
    "all_monthly_returns = all_monthly_returns.reset_index(level=1, drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d916994-eb24-4691-bee6-968627fb9252",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def handle_outliers(group1):\n",
    "    group = group1.copy()\n",
    "    non_nan_ind = ~np.any(np.isnan(group), axis=1)\n",
    "    group = group[non_nan_ind]\n",
    "    \n",
    "    for column in ['monthly Return']:\n",
    "        index = group[group[column] != 0].index\n",
    "        group2 = group.loc[index].copy()  # 创建一个副本以避免SettingWithCopyWarning\n",
    "        Q3 = group2[column].quantile(0.99)\n",
    "\n",
    "        upper_bound = Q3\n",
    "        \n",
    "        group2[column] = np.where((group2[column] > upper_bound), upper_bound, group2[column])\n",
    "        \n",
    "        group.loc[index, column] = group2[column]  # 使用.loc一次设置行和列\n",
    "    \n",
    "    group1.loc[non_nan_ind] = group\n",
    "    \n",
    "    return group1\n",
    "\n",
    "all_monthly_returns = all_monthly_returns.groupby(level=0, group_keys=False).apply(handle_outliers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81c9277a-d094-49ce-a6c1-c32b204304f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_monthly_rf(df_currency1):\n",
    "    \n",
    "    df_currency = df_currency1.copy()\n",
    "    \n",
    "    # 计算每日的累积回报\n",
    "    df_currency['Cumulative Periodic Return'] = (1 + df_currency['daily_return']).cumprod()\n",
    "    \n",
    "    rolling_return = df_currency['Cumulative Periodic Return'].rolling(window=30).apply(lambda x: x[-1]/x[0] - 1)\n",
    "    \n",
    "    # 创建一个新的DataFrame来存储这些回报\n",
    "    df_monthly_return = pd.DataFrame({\n",
    "        'monthly Return': rolling_return\n",
    "    })\n",
    "    \n",
    "    # 只为日期设置索引，不再为货币设置索引\n",
    "    df_monthly_return.index.name = 'Date'\n",
    "    \n",
    "    return df_monthly_return\n",
    "\n",
    "# 应用上述函数到每一个货币\n",
    "all_monthly_rf = compute_monthly_rf(rf_df)\n",
    "\n",
    "all_monthly_rf = all_monthly_rf.iloc[6:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70a35128-c6b4-4970-b9f4-5c3d61e9dbf6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "all_monthly_returns['excess_return'] = all_monthly_returns - all_monthly_rf.loc[pd.IndexSlice[all_monthly_returns.index.get_level_values('Date').unique()]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99fdacde-2a6d-4ffa-bfce-c607e7d72bdb",
   "metadata": {},
   "source": [
    "-----\n",
    "### monthly characteristics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35de4594-6039-47a5-85b2-0f7f1ebba3f3",
   "metadata": {},
   "source": [
    "### 5. size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bd7216f-7776-4f99-8eb5-a73a6d6d0a12",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_monthly_size(df_currency1):\n",
    "    \n",
    "    df_currency = df_currency1.copy()\n",
    "\n",
    "    # 重置日期索引\n",
    "    df_currency = df_currency.reset_index(level='Currency', drop=True)\n",
    "    \n",
    "    #monthly_size = np.log(df_currency['size'].rolling(window=30).apply(lambda x: x[-1])+1)\n",
    "    monthly_size = df_currency['size'].rolling(window=30).apply(lambda x: x[-1])\n",
    "    \n",
    "    \n",
    "    # 创建一个新的DataFrame来存储这些值\n",
    "    df_monthly_size = pd.DataFrame({\n",
    "        'size': monthly_size\n",
    "    })\n",
    "    \n",
    "    # 只为日期设置索引，不再为货币设置索引\n",
    "    df_monthly_size.index.name = 'Date'\n",
    "    \n",
    "    return df_monthly_size\n",
    "\n",
    "# 应用上述函数到每一个货币\n",
    "monthly_independ_variable_df = depend_variable_df.groupby(level='Currency').apply(compute_monthly_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bfff488-171c-41e8-b2a5-39dcaf4ef279",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "monthly_independ_variable_df = monthly_independ_variable_df.groupby(level='Currency').apply(lambda x: x.iloc[6:])\n",
    "monthly_independ_variable_df = monthly_independ_variable_df.reset_index(level=1, drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ab6ee59-22f7-4ed0-a338-8f0c03dd2d4e",
   "metadata": {},
   "source": [
    "### 1. new_add"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de043a96-9322-4202-a7fd-24e6767753eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_monthly_new_add(df_currency1):\n",
    "    \n",
    "    df_currency = df_currency1.copy()\n",
    "\n",
    "    # 重置日期索引\n",
    "    df_currency = df_currency.reset_index(level='Currency', drop=True)\n",
    "    \n",
    "    # 计算每周的new_addresses的平均值\n",
    "    #monthly_new_add = np.log(df_currency['new_addresses'].rolling(window=30).mean()+1)\n",
    "    monthly_new_add = df_currency['new_addresses'].rolling(window=30).sum()\n",
    "    \n",
    "    \n",
    "    # 创建一个新的DataFrame来存储这些值\n",
    "    df_monthly_new_add = pd.DataFrame({\n",
    "        'new_addresses': monthly_new_add\n",
    "    })\n",
    "    \n",
    "    # 只为日期设置索引，不再为货币设置索引\n",
    "    df_monthly_new_add.index.name = 'Date'\n",
    "    \n",
    "    return df_monthly_new_add\n",
    "\n",
    "# 应用上述函数到每一个货币\n",
    "all_monthly_new_add = depend_variable_df.groupby(level='Currency').apply(compute_monthly_new_add)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ea8f63d-5fc7-4ec0-9b3c-9ab95db4c88d",
   "metadata": {},
   "outputs": [],
   "source": [
    "monthly_independ_variable_df = monthly_independ_variable_df.join(all_monthly_new_add[['new_addresses']])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "905eda7b-128a-4937-862b-addc4b1d5702",
   "metadata": {},
   "source": [
    "### 2. active_add"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88b11daf-9b0a-408a-8f67-076e0c1f0bf4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def compute_monthly_act_add(df_currency1):\n",
    "    \n",
    "    df_currency = df_currency1.copy()\n",
    "\n",
    "    # 重置日期索引\n",
    "    df_currency = df_currency.reset_index(level='Currency', drop=True)\n",
    "    \n",
    "    # 计算每周的new_addresses的平均值\n",
    "    #monthly_act_add = np.log(df_currency['active_addresses'].rolling(window=30).apply(lambda x: x[-1])+1)\n",
    "    monthly_act_add = df_currency['active_addresses'].rolling(window=30).sum()\n",
    "    \n",
    "    \n",
    "    # 创建一个新的DataFrame来存储这些值\n",
    "    df_monthly_act_add = pd.DataFrame({\n",
    "        'active_addresses': monthly_act_add\n",
    "    })\n",
    "    \n",
    "    # 只为日期设置索引，不再为货币设置索引\n",
    "    df_monthly_act_add.index.name = 'Date'\n",
    "    \n",
    "    return df_monthly_act_add\n",
    "\n",
    "# 应用上述函数到每一个货币\n",
    "all_monthly_act_add = depend_variable_df.groupby(level='Currency').apply(compute_monthly_act_add)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e68859d0-840b-4e34-985b-f089b93e678f",
   "metadata": {},
   "outputs": [],
   "source": [
    "monthly_independ_variable_df = monthly_independ_variable_df.join(all_monthly_act_add[['active_addresses']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "472fffc6-9965-4c18-8ca1-c1a96e3fd2c0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def compute_monthly_variable(group1, variable_name):\n",
    "    group = group1.copy()  # 复制数据以防止修改原始数据\n",
    "    \n",
    "    # 重置日期索引\n",
    "    group = group.reset_index(level='Currency', drop=True)\n",
    "    \n",
    "    monthly_variable = group[variable_name].rolling(window=30).apply(lambda x: x[-1])\n",
    "    \n",
    "    # 创建一个新的DataFrame来存储这些值\n",
    "    df_monthly_variable = pd.DataFrame({\n",
    "        variable_name: monthly_variable\n",
    "    })\n",
    "    \n",
    "    # 只为日期设置索引，不再为货币设置索引\n",
    "    df_monthly_variable.index.name = 'Date'\n",
    "    \n",
    "    return df_monthly_variable\n",
    "\n",
    "def join_monthly_variable(main_df, group_df, variable_name):\n",
    "    all_monthly_variable = group_df.groupby(level='Currency').apply(lambda group: compute_monthly_variable(group, variable_name))\n",
    "    return main_df.join(all_monthly_variable[variable_name])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4317423-0b3c-4d75-9a95-0e5b51a6d74c",
   "metadata": {},
   "source": [
    "### 3. bm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81e47a67-5e9c-4a7c-aa6d-1e7a424b72d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "monthly_independ_variable_df = join_monthly_variable(monthly_independ_variable_df, top_50_currencies_df, 'network_to_market_value')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbec94c2-f271-470e-ba4f-c30fb4415b4b",
   "metadata": {},
   "source": [
    "### 4. volume"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aff3b3fa-6058-4611-ad41-cbaefb7581b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_monthly_volume(df_currency1):\n",
    "    \n",
    "    df_currency = df_currency1.copy()\n",
    "\n",
    "    # 重置日期索引\n",
    "    df_currency = df_currency.reset_index(level='Currency', drop=True)\n",
    "    \n",
    "    #monthly_volume = np.log(df_currency['volumeTotal'].rolling(window=30).mean()+1)\n",
    "    monthly_volume = df_currency['volumeTotal'].rolling(window=30).sum()\n",
    "    \n",
    "    # 创建一个新的DataFrame来存储这些值\n",
    "    df_monthly_volume = pd.DataFrame({\n",
    "        'volume': monthly_volume\n",
    "    })\n",
    "    \n",
    "    # 只为日期设置索引，不再为货币设置索引\n",
    "    df_monthly_volume.index.name = 'Date'\n",
    "    \n",
    "    return df_monthly_volume\n",
    "\n",
    "# 应用上述函数到每一个货币\n",
    "all_monthly_volume = depend_variable_df.groupby(level='Currency').apply(compute_monthly_volume)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a202dadc-3efb-4414-a842-28c364d1a5a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "monthly_independ_variable_df = monthly_independ_variable_df.join(all_monthly_volume[['volume']])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "876d8709-9fa9-4e30-bdb6-5d087d3db35d",
   "metadata": {},
   "source": [
    "### 6. rvol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05472ffb-439b-457d-8c4a-1578da349e92",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def first(x):\n",
    "    return x.iloc[0]\n",
    "\n",
    "def last(x):\n",
    "    return x.iloc[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5c33233-bc71-4c2d-9dd6-a038c4f92f1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_monthly_volatility(df1, N=2):\n",
    "    df_currency = df1.copy() \n",
    "\n",
    "    df_currency = df_currency.reset_index(level='Currency', drop=True)\n",
    "\n",
    "    # Resampling data to get monthly values\n",
    "    monthly = df_currency.rolling(window=30).agg({\n",
    "        'high': 'max',\n",
    "        'low': 'min',\n",
    "        'open': first,\n",
    "        'close': last\n",
    "    })\n",
    "        \n",
    "    monthly['log_co'] = np.log(monthly['close'] / monthly['open'])\n",
    "    monthly['log_oc'] = np.log(monthly['open'] / monthly['close'].shift(1))\n",
    "    monthly['log_ho'] = np.log(monthly['high'] / monthly['open'])\n",
    "    monthly['log_lo'] = np.log(monthly['low'] / monthly['open'])\n",
    "    monthly['log_hc'] = np.log(monthly['high'] / monthly['close'])\n",
    "    monthly['log_lc'] = np.log(monthly['low'] / monthly['close'])\n",
    "\n",
    "    monthly['log_ho'] = monthly['log_ho'].apply(lambda x: 0 if x < 0 else x)\n",
    "    monthly['log_lo'] = monthly['log_lo'].apply(lambda x: 0 if x > 0 else x)\n",
    "\n",
    "    monthly['vol_oc'] = monthly['log_oc'].rolling(N).std()\n",
    "    monthly['vol_co'] = monthly['log_co'].rolling(N).std()\n",
    "\n",
    "    monthly['vol_rs'] = np.sqrt(monthly[['log_ho', 'log_lo', 'log_hc', 'log_lc']].apply(lambda x: x[0]*x[2] + x[1]*x[3], axis=1).rolling(N).mean())\n",
    "    \n",
    "    k = 0.34 / (1.34 + (N+1) / (N-1))\n",
    "    monthly['vol_yz'] = np.sqrt(monthly['vol_co']**2 + k * monthly['vol_oc']**2 + (1-k) * monthly['vol_rs']**2)\n",
    "    \n",
    "     # 创建一个新的DataFrame来存储这些值\n",
    "    df_monthly_rvol = pd.DataFrame({\n",
    "        'rvol': monthly['vol_yz']\n",
    "    })\n",
    "    \n",
    "    # 只为日期设置索引，不再为货币设置索引\n",
    "    df_monthly_rvol.index.name = 'Date'\n",
    "    \n",
    "    return df_monthly_rvol\n",
    "\n",
    "N = 2\n",
    "\n",
    "volatility_monthly_df = top_50_currencies_df.groupby(level='Currency').apply(compute_monthly_volatility, N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a917f1dd-1f07-4708-8ce6-e335006d8775",
   "metadata": {},
   "outputs": [],
   "source": [
    "monthly_independ_variable_df = monthly_independ_variable_df.join(volatility_monthly_df[['rvol']])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c47bf183-9afd-44db-b615-d8e1fd4d65c5",
   "metadata": {},
   "source": [
    "### 7. bid-ask "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "937e1c91-e64e-4c6d-a764-52f39b333b06",
   "metadata": {},
   "source": [
    "#### 7.1 Abdi and Ranaldo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17321170-1474-4209-af62-daf914deeabf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def calculate_monthly_spread(df1):\n",
    "    df_currency = df1.copy() \n",
    "\n",
    "    df_currency = df_currency.reset_index(level='Currency', drop=True)\n",
    "\n",
    "    # Resampling data to get monthly values\n",
    "    monthly = df_currency.rolling(window=30).agg({\n",
    "        'high': 'max',\n",
    "        'low': 'min',\n",
    "        'open': first,\n",
    "        'close': last\n",
    "    })\n",
    "\n",
    "    monthly['log_close'] = np.log(monthly['close'])\n",
    "    monthly['log_mid'] = np.log((monthly['high'] + monthly['low']) / 2)\n",
    "    \n",
    "    # Using lagged mid and close to calculate spread\n",
    "    monthly['log_mid_lag'] = monthly['log_mid'].shift(1)\n",
    "    monthly['prod_diff'] = (monthly['log_close'] - monthly['log_mid']) * (monthly['log_close'] - monthly['log_mid_lag'])\n",
    "\n",
    "    # Checking if prod_diff is positive, if not set it to 0\n",
    "    monthly['prod_diff'] = monthly['prod_diff'].apply(lambda x: max(4*x, 0))\n",
    "    monthly['Spread_abdi'] = np.sqrt(monthly['prod_diff'])\n",
    "\n",
    "    # Creating a new DataFrame to store Spread values\n",
    "    df_monthly_spread = pd.DataFrame({\n",
    "        'Spread_abdi': monthly['Spread_abdi']\n",
    "    })\n",
    "    \n",
    "    # Only indexing by date, no longer indexing by currency\n",
    "    df_monthly_spread.index.name = 'Date'\n",
    "    \n",
    "    return df_monthly_spread\n",
    "\n",
    "bidask_monthly_df1 = top_50_currencies_df.groupby(level='Currency').apply(calculate_monthly_spread)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4961c4dd-8e9c-4491-b0db-b7ab01c5bfb0",
   "metadata": {},
   "source": [
    "#### 7.2 Corwin and Schultz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adbef983-1d20-4d53-b3a1-650e7bfd23cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_monthly_spread(df1):\n",
    "    # 创建一个新的数据框架的副本以防止对原始数据进行更改\n",
    "    df = df1.copy()\n",
    "    \n",
    "    # 重置货币索引以便于重新采样\n",
    "    df = df.reset_index(level='Currency', drop=True)\n",
    "    \n",
    "    # Resampling data to get monthly values\n",
    "    monthly = df.rolling(window=30).agg({\n",
    "        'high': 'max',\n",
    "        'low': 'min',\n",
    "        'open': first,\n",
    "        'close': last\n",
    "    })\n",
    "    \n",
    "    # 计算两天的最高和最低价\n",
    "    monthly['H_t_t+1'] = monthly['high'].rolling(2).max()\n",
    "    monthly['L_t_t+1'] = monthly['low'].rolling(2).min()\n",
    "\n",
    "    # 计算beta和gamma\n",
    "    monthly['beta'] = ((np.log(monthly['high'] / monthly['low']))**2).rolling(2).sum()\n",
    "    monthly['gamma'] = np.log(monthly['H_t_t+1'] / monthly['L_t_t+1'])**2\n",
    "\n",
    "    # 计算alpha和Spread\n",
    "    monthly['alpha'] = (np.sqrt(2 * monthly['beta']) - np.sqrt(monthly['beta'])) / (3 - 2 * np.sqrt(2)) - np.sqrt(monthly['gamma'] / (3 - 2 * np.sqrt(2)))\n",
    "    monthly['Spread_corwin'] = (2 * (np.exp(monthly['alpha']) - 1)) / (1 + np.exp(monthly['alpha']))\n",
    "    \n",
    "    monthly['Spread_corwin'] = monthly['Spread_corwin'].apply(lambda x: max(x, 0))\n",
    "\n",
    "    # 创建一个新的DataFrame来存储这些值\n",
    "    df_monthly_spread = pd.DataFrame({\n",
    "        'Spread_corwin': monthly['Spread_corwin']\n",
    "    })\n",
    "    \n",
    "    # 只为日期设置索引\n",
    "    df_monthly_spread.index.name = 'Date'\n",
    "    \n",
    "    return df_monthly_spread\n",
    "\n",
    "bidask_monthly_df2 = top_50_currencies_df.groupby(level='Currency').apply(calculate_monthly_spread)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "282f1939-db7f-46ef-bd90-85d8b47421e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "monthly_independ_variable_df = monthly_independ_variable_df.join(bidask_monthly_df2[['Spread_corwin']])\n",
    "monthly_independ_variable_df = monthly_independ_variable_df.join(bidask_monthly_df1[['Spread_abdi']])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd9d8d57-e423-42d1-9940-0166925e6e14",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 8. illiq "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "510185be-1129-482a-8abe-2dd635830259",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_series = abs(all_monthly_returns['monthly Return'].squeeze()) / monthly_independ_variable_df['volume']\n",
    "result_series.name = 'illiq' \n",
    "\n",
    "monthly_independ_variable_df = monthly_independ_variable_df.join(result_series)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb096d94-dd7b-4b11-bc88-c438e6193f68",
   "metadata": {},
   "outputs": [],
   "source": [
    "monthly_independ_variable_df['illiq'] = monthly_independ_variable_df.groupby('Currency')['illiq'].transform(lambda x: x.replace(np.inf, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3e433cb-04f1-42ec-8509-a16bed5cf5d8",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### 8.2 Zeros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b90e7b2d-45d8-47a0-abc5-198d68670843",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_monthly_zeros(df_currency1):\n",
    "    \n",
    "    df_currency = df_currency1.copy()\n",
    "\n",
    "    # 重置日期索引\n",
    "    df_currency = df_currency.reset_index(level='Currency', drop=True)\n",
    "    \n",
    "    monthly_zeros = df_currency['Zeros'].rolling(window=30).apply(lambda x: x[-1])\n",
    "    \n",
    "    # 创建一个新的DataFrame来存储这些值\n",
    "    df_monthly_zeros = pd.DataFrame({\n",
    "        'Zeros': monthly_zeros\n",
    "    })\n",
    "    \n",
    "    # 只为日期设置索引，不再为货币设置索引\n",
    "    df_monthly_zeros.index.name = 'Date'\n",
    "    \n",
    "    return df_monthly_zeros\n",
    "\n",
    "# 应用上述函数到每一个货币\n",
    "all_monthly_zeros = depend_variable_df.groupby(level='Currency').apply(compute_monthly_zeros)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3598182-1260-4eed-b755-b5986e8e6636",
   "metadata": {},
   "outputs": [],
   "source": [
    "monthly_independ_variable_df = monthly_independ_variable_df.join(all_monthly_zeros[['Zeros']])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90448fbb-1073-4166-bba0-245988e128ce",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 9. capm  beta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25b56b38-8da9-4a10-870b-438d901f40a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_monthly_capm(df_currency1):\n",
    "    \n",
    "    df_currency = df_currency1.copy()\n",
    "\n",
    "    # 重置日期索引\n",
    "    df_currency = df_currency.reset_index(level='Currency', drop=True)\n",
    "    \n",
    "    monthly_beta = df_currency['beta'].rolling(window=30).apply(lambda x: x[-1])\n",
    "    monthly_alpha = df_currency['alpha'].rolling(window=30).apply(lambda x: x[-1])\n",
    "    monthly_ivol = df_currency['ivol'].rolling(window=30).apply(lambda x: x[-1])\n",
    "    \n",
    "    # 创建一个新的DataFrame来存储这些值\n",
    "    df_monthly_capm = pd.DataFrame({\n",
    "        'beta': monthly_beta,\n",
    "        'alpha': monthly_alpha,\n",
    "        'ivol': monthly_ivol\n",
    "    })\n",
    "    \n",
    "    # 只为日期设置索引，不再为货币设置索引\n",
    "    df_monthly_capm.index.name = 'Date'\n",
    "    \n",
    "    return df_monthly_capm\n",
    "\n",
    "# 应用上述函数到每一个货币\n",
    "all_monthly_capm = depend_variable_df.groupby(level='Currency').apply(compute_monthly_capm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f114636c-c624-4593-9f21-e899c2c0bcdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "monthly_independ_variable_df = monthly_independ_variable_df.join(all_monthly_capm[['beta']])\n",
    "monthly_independ_variable_df = monthly_independ_variable_df.join(all_monthly_capm[['alpha']])\n",
    "monthly_independ_variable_df = monthly_independ_variable_df.join(all_monthly_capm[['ivol']])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97c7a95e-745c-4a83-ac1a-280e3598254d",
   "metadata": {},
   "source": [
    "### 12. turnover"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ac36b74-cc67-4840-82c5-d7ece674c5e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_monthly_turnover(df_currency1):\n",
    "    \n",
    "    df_currency = df_currency1.copy()\n",
    "\n",
    "    # 重置日期索引\n",
    "    df_currency = df_currency.reset_index(level='Currency', drop=True)\n",
    "    \n",
    "    monthly_turnover = df_currency['turnover'].rolling(window=30).mean()\n",
    "    \n",
    "    \n",
    "    # 创建一个新的DataFrame来存储这些值\n",
    "    df_monthly_turnover = pd.DataFrame({\n",
    "        'turnover': monthly_turnover\n",
    "    })\n",
    "    \n",
    "    # 只为日期设置索引，不再为货币设置索引\n",
    "    df_monthly_turnover.index.name = 'Date'\n",
    "    \n",
    "    return df_monthly_turnover\n",
    "\n",
    "# 应用上述函数到每一个货币\n",
    "all_monthly_turnover = depend_variable_df.groupby(level='Currency').apply(compute_monthly_turnover)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90efcf82-f4a1-4aa7-8034-28bf46551dc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "monthly_independ_variable_df = monthly_independ_variable_df.join(all_monthly_turnover['turnover'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba34d91c-8ea0-493f-a214-7ae91739b082",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 12.1. Turnover-Adjusted Number of Zero Daily Trading Volume"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c97f246-790d-4aa1-b00e-f959fff414e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "monthly_independ_variable_df = join_monthly_variable(monthly_independ_variable_df, depend_variable_df, 'turnover_adjusted')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d98d969-c219-4c9f-9395-24cddc78307c",
   "metadata": {},
   "source": [
    "### 13. dto    180 days Q!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeba7e24-f167-4566-b920-9ba00726b036",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "all_monthly_weight_turnover = pd.DataFrame(top_50_currencies_df['weight']*all_monthly_turnover['turnover'])\n",
    "monthly_market_turnover = all_monthly_weight_turnover.groupby(level='time', group_keys=False)[0].sum()\n",
    "monthly_market_turnover.name = 'market_turnover'\n",
    "monthly_market_turnover = monthly_market_turnover.rename_axis(index={'time': 'Date'})\n",
    "\n",
    "all_monthly_turnover = all_monthly_turnover.join(monthly_market_turnover)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59321334-aa9b-44cc-a8fe-913f82502c5f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 计算去趋势化的换手率\n",
    "def monthly_detrend_turnover(group1):\n",
    "    group = group1.copy()  # 复制数据以防止修改原始数据\n",
    "    \n",
    "    group['dto'] = group['turnover'] - group['market_turnover']\n",
    "    group['dto'] = group['dto'] - group['dto'].rolling(120).mean() \n",
    "    \n",
    "    return group\n",
    "\n",
    "all_monthly_dto = all_monthly_turnover.groupby(level='Currency', group_keys=False).apply(monthly_detrend_turnover)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d133a447-74c0-4752-9353-98d3b446d51c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "monthly_independ_variable_df = monthly_independ_variable_df.join(all_monthly_dto[['dto']])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0aecc5ba-dc76-4caf-85d0-dfa59c41e3c3",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 14. std_to"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94e7c96f-71cf-4c07-9071-e3e572a98d7e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "monthly_independ_variable_df = join_monthly_variable(monthly_independ_variable_df, depend_variable_df, 'std_to')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5005187d-3bc4-432d-adb5-0ee6fdec9aec",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 15. std_vol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bd697cf-12a1-43d2-8e01-08e72076fdd3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "monthly_independ_variable_df = join_monthly_variable(monthly_independ_variable_df, depend_variable_df, 'std_vol')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c35fab1e-e32a-48c1-a000-3fc177e29f8b",
   "metadata": {},
   "source": [
    "### 16. rel_to_high Q!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9144288f-ef41-4bd7-9ce6-881b1224b009",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "monthly_independ_variable_df = join_monthly_variable(monthly_independ_variable_df, depend_variable_df, 'rel_to_high')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a7e87d3-b6a1-419e-9904-19e80fcf64c4",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 17. max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fa09ee7-0f6f-425a-9cb0-4a4681c83f61",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "monthly_independ_variable_df = join_monthly_variable(monthly_independ_variable_df, depend_variable_df, 'max.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f56f4c3d-98ab-444b-a709-cdde5c4e3525",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 18. vol_shock"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af3e395c-bce2-4af2-8450-f0c0b75f087b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "monthly_independ_variable_df = join_monthly_variable(monthly_independ_variable_df, depend_variable_df, 'vol_shock_30d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21b88d07-49bc-4e94-ba8e-0ca0691b9c37",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "monthly_independ_variable_df = join_monthly_variable(monthly_independ_variable_df, depend_variable_df, 'vol_shock_60d')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1935de41-a23d-4df3-971c-f59c8187dece",
   "metadata": {},
   "source": [
    "### 19. rl,0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b2b8c6d-249e-4788-999c-232a587cb2dd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "monthly_independ_variable_df = join_monthly_variable(monthly_independ_variable_df, depend_variable_df, 'r2_1')\n",
    "monthly_independ_variable_df = join_monthly_variable(monthly_independ_variable_df, depend_variable_df, 'r7_2')\n",
    "monthly_independ_variable_df = join_monthly_variable(monthly_independ_variable_df, depend_variable_df, 'r13_2')\n",
    "\n",
    "monthly_independ_variable_df = join_monthly_variable(monthly_independ_variable_df, depend_variable_df, 'r22_2')\n",
    "monthly_independ_variable_df = join_monthly_variable(monthly_independ_variable_df, depend_variable_df, 'r31_2')\n",
    "monthly_independ_variable_df = join_monthly_variable(monthly_independ_variable_df, depend_variable_df, 'r30_14')\n",
    "monthly_independ_variable_df = join_monthly_variable(monthly_independ_variable_df, depend_variable_df, 'r180_60')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df1a9571-cfd8-461e-a90d-8435d28b67f1",
   "metadata": {},
   "source": [
    "### 26. VaR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35585d80-ccd2-4d85-b870-5a73332c67f4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "monthly_independ_variable_df = join_monthly_variable(monthly_independ_variable_df, depend_variable_df, 'VaR_5%')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b123b411-f8cb-45d7-b46d-a5d16b2e105d",
   "metadata": {},
   "source": [
    "# in sample "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7fb8ded-3d02-420c-8442-0db7ed58c83f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def latex_table_from_df(df):\n",
    "    header = r\"\"\"\n",
    "\\begin{table}\n",
    "\\centering\n",
    "\\caption{Non-overlapping monthly characteristics from Feb 2018 to Aug 2023}\n",
    "\\begin{tabular}{|c|c|c|c|c|c|c|c|c|c|c|}\n",
    "\\hline & \\multirow[b]{2}{*}{\\text { Obs. }} & \\multirow[b]{2}{*}{\\text { Mean }} & \\multirow[b]{2}{*}{\\text { Median }} & \\multirow[b]{2}{*}{\\text { Std }} & \\multicolumn{6}{|c|}{\\text { Percentiles }} \\\\\n",
    "\\hline & & & & & 1 & 5 & 25 & 75 & 95 & 99 \\\\\n",
    "\\hline\"\"\"\n",
    "\n",
    "    rows = []\n",
    "\n",
    "    for col in df.columns:\n",
    "        # Handle math symbols in column names\n",
    "        col_name = col.replace('_', '\\_')\n",
    "        col_name = col_name.replace('%', '\\%')\n",
    "        if any(symbol in col for symbol in ['^', '_', '{', '}', '%']):\n",
    "            col_name = r'$' + col_name + r'$'\n",
    "\n",
    "        obs = df[col].count()\n",
    "        mean_val = df[col].mean()\n",
    "        median_val = df[col].median()\n",
    "        std_val = df[col].std()\n",
    "        percentiles = np.percentile(df[col].dropna(), [1, 5, 25, 75, 95, 99])\n",
    "\n",
    "        row = f\"{col_name} & {obs} & {mean_val:.2f} & {median_val:.2f} & {std_val:.2f}\"\n",
    "        row += \" & \" + \" & \".join([f\"{p:.2f}\" for p in percentiles]) + r\" \\\\ \\hline\"\n",
    "        rows.append(row)\n",
    "\n",
    "    footer = r\"\\end{tabular} \\end{table}\"\n",
    "\n",
    "    table = header + \"\\n\" + \"\\n\".join(rows) + \"\\n\" + footer\n",
    "    return table\n",
    "\n",
    "\n",
    "# 假设 sliced_df_scaled1.loc[non_nan_ind] 是您的数据\n",
    "df = sliced_df_scaled1.loc[non_nan_ind]\n",
    "\n",
    "print(latex_table_from_df(df))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91b8f0a6-85e0-41cb-b8b2-b4ab9426970f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sliced_returns = all_monthly_returns['excess_return'].copy()\n",
    "sliced_df_scaled1 = monthly_independ_variable_df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ece0d80-1b65-4b7e-b900-42be0bd18957",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def sample_every_6_days(group):\n",
    "    return group.iloc[::30]\n",
    "\n",
    "sliced_returns = all_monthly_returns.groupby(level='Currency').apply(sample_every_6_days).reset_index(level=0, drop=True)['excess_return']\n",
    "sliced_df_scaled1 = monthly_independ_variable_df.groupby(level='Currency').apply(sample_every_6_days).reset_index(level=0, drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d8879e2-b6d1-4431-b626-7f4d5095bad4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def cross_sectional_normalization(df1):\n",
    "    df = df1.copy()\n",
    "    \"\"\"\n",
    "    Cross-sectionally normalize a DataFrame with multi-index (Currency, time).\n",
    "    The function will rank the values, then divide ranks by the number of non-missing observations \n",
    "    and subtract 0.5, mapping values to the interval [-0.5, +0.5].\n",
    "    \n",
    "    Parameters:\n",
    "    - df: DataFrame with multi-index (Currency, time)\n",
    "\n",
    "    Returns:\n",
    "    - DataFrame with the same indices but transformed values.\n",
    "    \"\"\"\n",
    "    \n",
    "    # This function will be applied to each period to perform the transformation\n",
    "    def period_normalization(group):\n",
    "        non_missing_count = group.count()\n",
    "        \n",
    "        # We will use a temporary DataFrame to maintain the original structure\n",
    "        temp_df = pd.DataFrame(index=group.index)\n",
    "        temp_df['rank'] = group.rank()\n",
    "        \n",
    "        # Normalize ranks\n",
    "        temp_df['rank'] = temp_df['rank'] / non_missing_count - 0.5\n",
    "        \n",
    "        # Fill in the ranks; if NaN in original, it will remain NaN in the result\n",
    "        group[:] = temp_df['rank']\n",
    "        return group\n",
    "\n",
    "    non_nan_ind = ~np.any(np.isnan(df), axis=1)\n",
    "    df1 = df.loc[non_nan_ind]\n",
    "    # Group by 'time' level of the index and apply the transformation\n",
    "    df.loc[non_nan_ind] = df1.groupby(level='Date').transform(period_normalization)\n",
    "    \n",
    "    return df\n",
    "\n",
    "df_scaled  = cross_sectional_normalization(sliced_df_scaled1)\n",
    "\n",
    "sliced_df_scaled1 = df_scaled.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e98a0344-6cb6-4443-a781-5a358853784f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sliced_df_scaled1[['size', 'new_addresses', 'active_addresses', 'volume', 'illiq', 'std_to', 'std_vol', 'turnover_adjusted']] = np.log1p(sliced_df_scaled1[['size', 'new_addresses', 'active_addresses', 'volume', 'illiq', 'std_to', 'std_vol', 'turnover_adjusted']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61294efb-ee4b-4ff7-ae6a-c4a86715afb7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "minmax_scaler = MinMaxScaler(feature_range=(-0.5, 0.5))\n",
    "non_nan_ind = ~np.any(np.isnan(sliced_df_scaled1), axis=1)\n",
    "sliced_df_scaled1.loc[non_nan_ind] = minmax_scaler.fit_transform(sliced_df_scaled1[non_nan_ind])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78f5c106-2cd3-49a8-8fcc-12699ec535df",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "non_nan_ind = ~np.any(np.isnan(sliced_df_scaled1), axis=1)\n",
    "sliced_df_scaled1[non_nan_ind].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a43cf6d8-06a0-4c06-8489-55bd32ca7a18",
   "metadata": {
    "tags": []
   },
   "source": [
    "### trainning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15a140be-773a-4345-965d-39f86f64e9e6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import my_ipca\n",
    "\n",
    "importlib.reload(my_ipca)\n",
    "\n",
    "from my_ipca import InstrumentedPCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c447392-69ad-4a2f-9150-5a70509b889b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def calculate_metric(X, y, pred, n):\n",
    "    \n",
    "    non_nan_ind = ~np.any(np.isnan(X), axis=1)\n",
    "    df = y.loc[non_nan_ind]\n",
    "    \n",
    "    # 重置索引以确保\"Currency\"和\"time\"是列\n",
    "    df_reset = df.reset_index()\n",
    "    \n",
    "    # 用 pivot 方法重塑数据\n",
    "    pivot_df = df_reset.pivot(index='Date', columns='Currency')\n",
    "    \n",
    "    # 现在，pivot_df 的每一行都应该代表一个时间点，列代表不同的资产。\n",
    "    # 没有数据的地方将会是 NaN\n",
    "    list_of_lists = pivot_df.values.tolist()\n",
    "    y_array = np.array(list_of_lists) # return listed by date\n",
    "    \n",
    "    diff = 0\n",
    "    sum1 = 0\n",
    "\n",
    "    #print(pred[n-1])\n",
    "    for i in range(n, len(list_of_lists)):\n",
    "        diff += np.nansum((y_array[i] - pred[i]) ** 2)\n",
    "        sum1 += np.nansum(y_array[i] ** 2)\n",
    "    \n",
    "    return 1 - diff / sum1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04e94aee-59b4-432a-91c1-5c7613f4e252",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def predFactor(regr, X, y):\n",
    "    Gamma, Factors = regr.get_factors(label_ind=True)\n",
    "    models = {}\n",
    "    lambda_t_dict = {}\n",
    "    df = Factors.T\n",
    "    \n",
    "    # 计算 lambda_t\n",
    "    for col in df.columns:\n",
    "        print(f\"Fitting {col}...\")\n",
    "        model = auto_arima(df[col],\n",
    "                           start_p=0, start_q=0,\n",
    "                           max_p=30, max_q=30,\n",
    "                           seasonal=False,\n",
    "                           trace=True,\n",
    "                           error_action='ignore',\n",
    "                           suppress_warnings=True)\n",
    "        models[col] = model\n",
    "        forecast = model.predict_in_sample()\n",
    "        lambda_t_dict[col] = forecast\n",
    "    \n",
    "    lambda_t = pd.DataFrame(lambda_t_dict)\n",
    "    \n",
    "    # 为每个货币进行计算\n",
    "    est_ret_dict = {}\n",
    "    y_dict = {}\n",
    "    \n",
    "    for currency in X.index.levels[0]:\n",
    "        print(f\"Processing {currency}...\")\n",
    "        \n",
    "        # 基于 BTC 的 non_nan_ind 截取当前货币的 DataFrame\n",
    "        non_nan_ind = ~np.any(np.isnan(X.loc['BTC']), axis=1)\n",
    "        X_trimmed = X.loc[currency][non_nan_ind]\n",
    "        y_trimmed = y.loc[currency][non_nan_ind]\n",
    "        \n",
    "        # 计算预测回报\n",
    "        est_ret_list = []  # 初始化一个空列表来存储每一个时间点的预测回报\n",
    "        for t in X_trimmed.index:\n",
    "            x_t = X_trimmed.loc[t]  # 1xN\n",
    "            lambda_t_t = lambda_t.loc[t]  # Kx1\n",
    "            est_ret_t = x_t.dot(Gamma).dot(lambda_t_t)  # 1xN * NxK * Kx1 = 1x1\n",
    "            est_ret_list.append(est_ret_t)\n",
    "        \n",
    "        est_ret_series = pd.Series(est_ret_list, index=X_trimmed.index)  # 转换为 Series\n",
    "        est_ret_dict[currency] = est_ret_series\n",
    "        y_dict[currency] = y_trimmed\n",
    "    \n",
    "    # 合并所有货币的预测回报到一个 DataFrame\n",
    "    est_ret_df = pd.concat(est_ret_dict, axis=1)\n",
    "    y_trimmed = pd.concat(y_dict, axis=1)\n",
    "    \n",
    "    non_nan_ind = ~np.isnan(est_ret_df.T)\n",
    "    \n",
    "    result = 1-np.nansum((est_ret_df.T-y_trimmed.T)**2)/np.nansum((y_trimmed.T[non_nan_ind])**2)\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9469920-20f2-41be-aae9-af7eb1f67e72",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 为每个n_factors值创建一个字典来保存整个regr对象\n",
    "results_in_monthly_30_minmax_full = {}\n",
    "\n",
    "for i in range(1, 7):  # for n_factors from 1 to 6\n",
    "    \n",
    "    X = sliced_df_scaled1\n",
    "    y = sliced_returns\n",
    "    \n",
    "    regr = InstrumentedPCA(n_factors=i, intercept=False, max_iter=2000, n_jobs=-1, iter_tol=10e-4)\n",
    "    regr = regr.fit(X=X, y=y)\n",
    "    \n",
    "    r2_pred = predFactor(regr, X, y)\n",
    "    \n",
    "    pred = regr.predict(X=X)\n",
    "    pred_m = regr.predict(X=X, mean_factor=True)\n",
    "      \n",
    "    non_nan_ind = ~np.any(np.isnan(X), axis=1)\n",
    "    \n",
    "    y = y[non_nan_ind].values\n",
    "    r2_total =  1-(np.sum((y - pred)**2)/ np.sum(y**2))\n",
    "    r2_pred_m = 1-(np.sum((y - pred_m)**2)/ np.sum(y**2))\n",
    "\n",
    "    # 执行 Ljung-Box 检验\n",
    "    ljung_box_results_list = []\n",
    "    for j in range(i):  # 因为你有 i 个因子\n",
    "        Gamma, Factors = regr.get_factors(label_ind=True)\n",
    "        ljung_box_results = acorr_ljungbox(Factors.loc[j], lags=[10])\n",
    "        ljung_box_results_list.append(ljung_box_results)\n",
    "    \n",
    "    # 存储结果\n",
    "    results_in_monthly_30_minmax_full[i] = {'model': regr}\n",
    "    results_in_monthly_30_minmax_full[i]['r2_total'] = r2_total\n",
    "    results_in_monthly_30_minmax_full[i]['r2_pred'] = r2_pred\n",
    "    results_in_monthly_30_minmax_full[i]['r2_predm'] = r2_pred_m\n",
    "    results_in_monthly_30_minmax_full[i]['ljung_box_results'] = ljung_box_results_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58595377-90b7-453b-b81f-8fb082c79861",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_in_monthly_minmax_slice[3]['r2_total']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19357a6b-a7de-4a61-9d22-1808e90f4134",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "results_in_monthly_minmax_slice[5]['r2_predm']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4777568d-fb35-4421-ac7b-4d8483573872",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 保存词典到文件\n",
    "with open('month_in_30_minmax_full.pickle', 'wb') as handle:\n",
    "    pickle.dump(results_in_monthly_30_minmax_full, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09eb2b6a-2f1d-47cb-bfd9-d39d5da9054e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from statsmodels.stats.diagnostic import acorr_ljungbox\n",
    "ljung_box_results = acorr_ljungbox(Factors.loc[0], lags=[10])\n",
    "ljung_box_results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acac50b1-54b8-45ec-ab01-5a4c906101fc",
   "metadata": {
    "tags": []
   },
   "source": [
    "# OOS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c53f0639-34d3-4b07-92e1-2c7b5690e4b1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sliced_returns = all_monthly_returns['excess_return'].copy()\n",
    "sliced_df = monthly_independ_variable_df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f96d4195-1f3e-4a9f-a4a2-49205844f8cf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def sample_every_6_days(group):\n",
    "    return group.iloc[::30]\n",
    "\n",
    "sliced_returns_df = all_monthly_returns.groupby(level='Currency').apply(sample_every_6_days).reset_index(level=0, drop=True)\n",
    "sliced_returns = sliced_returns_df['excess_return'].copy()\n",
    "sliced_df = monthly_independ_variable_df.groupby(level='Currency').apply(sample_every_6_days).reset_index(level=0, drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e968dd4-1eca-4fda-8ea1-9c6bebe42863",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def cross_sectional_normalization(df1):\n",
    "    df = df1.copy()\n",
    "    \"\"\"\n",
    "    Cross-sectionally normalize a DataFrame with multi-index (Currency, time).\n",
    "    The function will rank the values, then divide ranks by the number of non-missing observations \n",
    "    and subtract 0.5, mapping values to the interval [-0.5, +0.5].\n",
    "    \n",
    "    Parameters:\n",
    "    - df: DataFrame with multi-index (Currency, time)\n",
    "\n",
    "    Returns:\n",
    "    - DataFrame with the same indices but transformed values.\n",
    "    \"\"\"\n",
    "    \n",
    "    # This function will be applied to each period to perform the transformation\n",
    "    def period_normalization(group):\n",
    "        non_missing_count = group.count()\n",
    "        \n",
    "        # We will use a temporary DataFrame to maintain the original structure\n",
    "        temp_df = pd.DataFrame(index=group.index)\n",
    "        temp_df['rank'] = group.rank()\n",
    "        \n",
    "        # Normalize ranks\n",
    "        temp_df['rank'] = temp_df['rank'] / non_missing_count - 0.5\n",
    "        \n",
    "        # Fill in the ranks; if NaN in original, it will remain NaN in the result\n",
    "        group[:] = temp_df['rank']\n",
    "        return group\n",
    "\n",
    "    non_nan_ind = ~np.any(np.isnan(df), axis=1)\n",
    "    df1 = df.loc[non_nan_ind]\n",
    "    # Group by 'time' level of the index and apply the transformation\n",
    "    df.loc[non_nan_ind] = df1.groupby(level='Date').transform(period_normalization)\n",
    "    \n",
    "    return df\n",
    "\n",
    "sliced_df  = cross_sectional_normalization(sliced_df).copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb380db6-dba2-47c4-b063-6af4806ed213",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sliced_df[['size', 'new_addresses', 'active_addresses', 'volume', 'illiq', 'std_vol', 'turnover_adjusted']] = np.log1p(sliced_df[['size', 'new_addresses', 'active_addresses', 'volume', 'illiq', 'std_vol', 'turnover_adjusted']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4456161-d87d-4c56-a1a3-27a0f39b9447",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "non_nan_ind = ~np.any(np.isnan(sliced_df), axis=1)\n",
    "sliced_df.loc[non_nan_ind].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9660ac0d-d4e8-4f6d-9989-13d60e8cd781",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Step 1: Filter for rows corresponding to 'BTC' the longest coin\n",
    "valid_df = sliced_df.loc['BTC'].copy()\n",
    "\n",
    "# Step 2: Determine the first time a non-NaN value appears for each column\n",
    "first_valid_indices = valid_df.apply(lambda col: col.first_valid_index())\n",
    "\n",
    "idx = max(first_valid_indices)\n",
    "\n",
    "first_valid_idx = idx\n",
    "\n",
    "window_size = 30*33\n",
    "step_size = 1*30\n",
    "#valid_date = 60\n",
    "\n",
    "# Initialize the scaler\n",
    "minmax_scaler = MinMaxScaler(feature_range=(-0.5, 0.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5944c749-5dab-402c-9fd4-d0f8c0bd627d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def forecastFactor(regr, X_t, n):\n",
    "    Gamma, Factors = regr.get_factors(label_ind=True)\n",
    "    models = {}  # 用于存储每个时间序列的最佳模型\n",
    "    lambda_t = []\n",
    "    df = Factors.T\n",
    "    \n",
    "    for col in df.columns:\n",
    "        print(f\"Fitting {col}...\")\n",
    "        model = auto_arima(df[col],\n",
    "                           start_p=0, start_q=0,\n",
    "                           max_p=30, max_q=30,\n",
    "                           seasonal=False,\n",
    "                           trace=True,\n",
    "                           error_action='ignore',\n",
    "                           suppress_warnings=True)\n",
    "        models[col] = model\n",
    "        \n",
    "        n_periods = n  # 单步预测\n",
    "        forecast, conf_int = model.predict(n_periods=n_periods, return_conf_int=True)\n",
    "        lambda_t.append(forecast)\n",
    "    \n",
    "    values = [s.iloc[0] for s in lambda_t]\n",
    "    \n",
    "    est_ret = X_t.dot(Gamma).dot(pd.DataFrame(values)).iloc[:, 0].tolist()\n",
    "        \n",
    "    return est_ret, values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08fbd3db-bf40-4732-9edb-e09872994b14",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def forecastreturn(regr, X_t, n):\n",
    "    Gamma, Factors = regr.get_factors(label_ind=True)\n",
    "    lambda_t = []\n",
    "    df = Factors.T\n",
    "    k = 3\n",
    "    \n",
    "    model = Sequential()\n",
    "    model.add(LSTM(50, activation='relu', input_shape=(k*n, 1), return_sequences=True))\n",
    "    model.add(LSTM(25, activation='relu'))  # Additional LSTM layer\n",
    "    model.add(Dense(1))\n",
    "    model.compile('adam', loss='mae')\n",
    "    \n",
    "    early_stop = EarlyStopping(monitor='val_loss', patience=10)  # Early stopping\n",
    "        \n",
    "    for col in df.columns:\n",
    "        print(f\"Fitting {col}...\")\n",
    "        factor = np.array(df[col]).reshape(-1, 1)\n",
    "        X, y = [], []\n",
    "        for i in range(k*n, len(factor) - n + 1):\n",
    "            X.append(factor[i-k*n:i])\n",
    "            y.append(factor[i+n-1])\n",
    "        X = np.array(X)\n",
    "        y = np.array(y)\n",
    "            \n",
    "        model.fit(X, y, epochs=200, batch_size=32, verbose=0, callbacks=[early_stop], validation_split=0.2)\n",
    "        \n",
    "        x_input = np.array(factor[-k*n:]).reshape((1, k*n, 1))\n",
    "        yhat = model.predict(x_input)\n",
    "        lambda_t.append(yhat[0][0])\n",
    "    print(Factors.iloc[:,-1])\n",
    "        \n",
    "    est_ret = X_t.dot(Gamma).dot(pd.DataFrame(lambda_t)).iloc[:, 0].tolist()\n",
    "    \n",
    "    return est_ret, pd.DataFrame(lambda_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43b85777-d340-4b17-83af-f8567bfb06b2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train_svr_for_factor(factor_data, kernel='rbf', C=100, epsilon=0.1):\n",
    "    # 使用前n个值作为输入，预测第n+1个值\n",
    "    n = 1\n",
    "    k = 2\n",
    "    X = [factor_data[i-k*n:i] for i in range(k*n, len(factor_data)-n+1)]\n",
    "    y = factor_data[k*n:]\n",
    "\n",
    "    # 训练SVR模型\n",
    "    svr = SVR(kernel=kernel, C=C, epsilon=epsilon)\n",
    "    svr.fit(X, y)\n",
    "    \n",
    "    X_pred = [factor_data[-k*n:]]\n",
    "    y_pred = svr.predict(X_pred)\n",
    "\n",
    "    return y_pred[0]\n",
    "\n",
    "\n",
    "def forecastSVM(regr, X_t, n):\n",
    "    Gamma, Factors = regr.get_factors(label_ind=True)\n",
    "    lambda_t = []\n",
    "    df = Factors.T\n",
    "    \n",
    "    for col in df.columns:\n",
    "        print(f\"Fitting {col}...\")\n",
    "        yhat = train_svr_for_factor(df[col].values, kernel='rbf', C=1, epsilon=0.1)\n",
    "        lambda_t.append(yhat)\n",
    "    est_ret = X_t.dot(Gamma).dot(pd.DataFrame(lambda_t)).iloc[:, 0].tolist()\n",
    "    \n",
    "    return est_ret, pd.DataFrame(lambda_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41bbd9d5-7d35-4e82-a7b1-e3bd517fea3f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def optimize_portfolio_sharpe(expected_excess_return, cov_matrix, risk_budget=None, min_weight=0.01, max_weight=0.2):\n",
    "    n = len(expected_excess_return)\n",
    "\n",
    "    N = 50\n",
    "    qs = [10**(-5.0 * t/N + 1.0) for t in range(N)]\n",
    "    \n",
    "    # 设置目标函数参数（-Sharpe ratio，cvxopt是求最小值，因此加负号）\n",
    "    P = matrix(cov_matrix)\n",
    "    pbar = matrix(expected_excess_return)\n",
    "    \n",
    "    # 设置不等式约束（Gx <= h）\n",
    "    # 包括权重界限约束：min_weight <= w <= max_weight\n",
    "    G = matrix(np.concatenate([np.identity(n), -np.identity(n)], axis=0))\n",
    "    h_values = np.concatenate([np.ones(n) * max_weight, np.ones(n) * -min_weight])\n",
    "    h = matrix(h_values)\n",
    "    \n",
    "    # 设置等式约束（Ax = b）\n",
    "    # 权重之和为1\n",
    "    A = matrix(np.ones((1, n)).astype(float))\n",
    "    b = matrix(1.0)\n",
    "    \n",
    "    # 解优化问题\n",
    "    weights = [solvers.qp(P, -q*pbar, G, h, A, b)['x'] for q in qs]\n",
    "    \n",
    "    returns = [blas.dot(pbar, x) for x in weights]\n",
    "    risks = [np.sqrt(blas.dot(x, P * x)) for x in weights]\n",
    "    \n",
    "    sharpe_ratio = [r/risk for r, risk in zip(returns, risks)]\n",
    "    \n",
    "    index = max((val, idx) for idx, val in enumerate(sharpe_ratio))[1]\n",
    "    \n",
    "    optimal_weights = weights[index]\n",
    "\n",
    "    return optimal_weights, sharpe_ratio[index]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f47f59f2-c8cc-427b-9f09-5c9ab51c2516",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def calculate_cov(regr, X_t, window_size):\n",
    "    Gamma, Factors = regr.get_factors(label_ind=True)\n",
    "    \n",
    "    cov_factor = (Factors.iloc[:, -window_size:].T).cov() # using last 90 day to calculate covariance\n",
    "    beta_t = X_t.dot(Gamma)\n",
    "    \n",
    "    cov_array = beta_t.dot(cov_factor).dot(beta_t.T).values\n",
    "    \n",
    "    return cov_array    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23a85666-b59d-4a69-8fa9-c5a70c5073e7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "results_oos_month_rank_slice_svm = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a85c59c6-52d2-46e6-9384-ea9aae12272c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for i in range(1,7):  # for n_factors from 1 to 6\n",
    "    sum_diff = 0\n",
    "    sum_diff_pre = 0\n",
    "    sum_r2 = 0\n",
    "    k = 1\n",
    "    \n",
    "    start_date = first_valid_idx\n",
    "    next_date = pd.Timedelta(days=window_size) + start_date\n",
    "    end_date = monthly_independ_variable_df.index.get_level_values('Date').max()\n",
    "    next_date1 = pd.Timedelta(days=30) + next_date\n",
    "    \n",
    "    predicted_values_df = pd.DataFrame(index=sliced_df.index, columns=['predicted_value'])\n",
    "    portfolio_sharpe_ratio_df = pd.DataFrame(index=all_monthly_rf.index, columns=['sharpe_ratio'])\n",
    "\n",
    "    while next_date <= end_date and (pd.Timedelta(days=30) + next_date) <= end_date:\n",
    "        \n",
    "        print(\"###################:\", next_date)\n",
    "        print('loop:',i)\n",
    "        unique_currencies1 = (sliced_df.loc[pd.IndexSlice[:, next_date], :].dropna()).index.get_level_values('Currency').unique()\n",
    "        next_date1 = pd.Timedelta(days=30) + next_date\n",
    "        unique_currencies2 = (sliced_df.loc[pd.IndexSlice[:, next_date1], :].dropna()).index.get_level_values('Currency').unique()\n",
    "        \n",
    "        # 将两个 unique_currencies Pandas Index 对象转换为 set\n",
    "        set_unique_currencies1 = set(unique_currencies1)\n",
    "        set_unique_currencies2 = set(unique_currencies2)\n",
    "        \n",
    "        # 找到两个集合中的共有元素\n",
    "        common_currencies = set_unique_currencies1.intersection(set_unique_currencies2)\n",
    "        \n",
    "        unique_currencies = pd.Index(list(common_currencies)).sort_values()\n",
    "        \n",
    "        # Slice and clean data\n",
    "        \n",
    "        sliced_df_scaled2 = sliced_df.loc[pd.IndexSlice[unique_currencies, start_date:next_date], :].sort_index().copy()\n",
    "        return_series_oos = sliced_returns.loc[pd.IndexSlice[unique_currencies, start_date:next_date]].sort_index().copy()\n",
    "        \n",
    "        non_nan_ind = ~np.any(np.isnan(sliced_df_scaled2), axis=1)\n",
    "        #sliced_df_scaled2[non_nan_ind] = minmax_scaler.fit_transform(sliced_df_scaled2[non_nan_ind])\n",
    "        \n",
    "        X = sliced_df_scaled2\n",
    "        y = return_series_oos\n",
    "        regr = InstrumentedPCA(n_factors=i, intercept=False, max_iter=5000,n_jobs=-1,iter_tol=0.0005)\n",
    "        regr = regr.fit(X=X, y=y)\n",
    "        \n",
    "        # data for prediction next day\n",
    "        sliced_df_scaled2 = sliced_df.loc[pd.IndexSlice[unique_currencies, start_date:next_date1], :].sort_index().copy()\n",
    "        return_series_oos = sliced_returns.loc[pd.IndexSlice[unique_currencies, start_date:next_date1]].sort_index().copy()\n",
    "\n",
    "        # scale X\n",
    "        non_nan_ind = ~np.any(np.isnan(sliced_df_scaled2), axis=1)\n",
    "        #sliced_df_scaled2[non_nan_ind] = minmax_scaler.transform(sliced_df_scaled2[non_nan_ind])\n",
    "        \n",
    "        X = sliced_df_scaled2\n",
    "        y = return_series_oos\n",
    "        y_returns = regr.predictOOS(X=X, y=y, mean_factor=False,n=k) # predict next periods\n",
    "        y_pred = y_returns[-1] # last prediction\n",
    "        \n",
    "        X_t = sliced_df_scaled2.loc[pd.IndexSlice[unique_currencies, next_date],:].sort_index()\n",
    "        \n",
    "        #y_predm, e_ft1 = forecastreturn(regr, X_t, n=k)\n",
    "        y_predm = regr.predictOOS(X=X, y=y, mean_factor=True,n=k)[-1]\n",
    "        #y_predm, e_ft1 = forecastFactor(regr, X_t, n=k)\n",
    " \n",
    "\n",
    "        #non_nan_ind = ~np.any(np.isnan(X), axis=1)\n",
    "        y = sliced_returns.loc[pd.IndexSlice[unique_currencies, next_date1]].sort_index().values\n",
    "        \n",
    "        #non_nan_count = np.sum(~np.isnan(y_returns), axis=0)\n",
    "        #indices = np.where(non_nan_count > 20)[0]\n",
    "        #portfolio_currencies = [unique_currencies[i] for i in indices]\n",
    "\n",
    "        portfolio_return = np.nan\n",
    "        \n",
    "        if len(unique_currencies) < 20:\n",
    "            # Handle the special case when portfolio_currencies is empty\n",
    "            print(\"No suitable currencies found for portfolio for date:\", next_date1)\n",
    "        else:         \n",
    "            cov_matrix = calculate_cov(regr, X_t, window_size=15)\n",
    "            #expected_returns = [y_pred[i] for i in indices]\n",
    "            expected_returns = y_predm\n",
    "            \n",
    "            optimal_weights, sharpe_ratio = optimize_portfolio_sharpe(expected_returns, cov_matrix, risk_budget=None, min_weight=0.01, max_weight=0.2)\n",
    "            predicted_values_df.loc[(unique_currencies, next_date1), 'optimal_weights'] = optimal_weights\n",
    "            portfolio_sharpe_ratio_df.loc[next_date1, 'sharpe_ratio'] = sharpe_ratio\n",
    "            portfolio_return = np.sum(y*predicted_values_df.loc[(unique_currencies, next_date1), 'optimal_weights'])\n",
    "            \n",
    "        print(portfolio_return)\n",
    "        \n",
    "        predicted_values_df.loc[(unique_currencies, next_date1), 'predicted_value'] = y_pred\n",
    "        predicted_values_df.loc[(unique_currencies, next_date1), 'predicted_mean_value'] = y_predm\n",
    "        predicted_values_df.loc[(unique_currencies, next_date1), 'portfolio_return'] = portfolio_return\n",
    "    \n",
    "        \n",
    "        sum_diff += np.sum((y - y_pred)**2)\n",
    "        sum_diff_pre += np.sum((y - y_predm)**2)\n",
    "        sum_r2 += np.sum(y ** 2)\n",
    "        \n",
    "        print(1 - (sum_diff/sum_r2))\n",
    "        print(1 - (sum_diff_pre/sum_r2))\n",
    "        print(\"###################\")\n",
    "          \n",
    "        next_date += pd.Timedelta(days=step_size)\n",
    "    \n",
    "                           \n",
    "    results_oos_month_rank_slice_svm[i] = {'model': regr}\n",
    "    results_oos_month_rank_slice_svm[i]['r2_total'] = 1 - (sum_diff/sum_r2)\n",
    "    results_oos_month_rank_slice_svm[i]['r2_pred'] = 1 - (sum_diff_pre/sum_r2)\n",
    "    results_oos_month_rank_slice_svm[i]['pred'] = predicted_values_df\n",
    "    results_oos_month_rank_slice_svm[i]['sharpe'] = portfolio_sharpe_ratio_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b83e7c4-d09b-4299-9b7e-1fc97f9d87e9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "results_oos_month_rank_slice_svm[6]['r2_pred']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47075674-2fd8-4ca9-8d43-b8d84f595cb3",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "for i in range(1,2):  # for n_factors from 1 to 6\n",
    "    sum_diff = 0\n",
    "    sum_diff_pre1 = 0\n",
    "    sum_diff_pre2 = 0\n",
    "    sum_diff_pre3 = 0\n",
    "    sum_r2 = 0\n",
    "    k = 1\n",
    "    \n",
    "    start_date = first_valid_idx\n",
    "    next_date = pd.Timedelta(days=window_size) + start_date\n",
    "    end_date = monthly_independ_variable_df.index.get_level_values('Date').max()\n",
    "    next_date1 = pd.Timedelta(days=30) + next_date\n",
    "    \n",
    "    predicted_values_df = pd.DataFrame(index=sliced_df.index, columns=['predicted_value'])\n",
    "    portfolio_sharpe_ratio_df = pd.DataFrame(index=all_monthly_rf.index, columns=['sharpe_ratio'])\n",
    "\n",
    "    while next_date <= end_date and (pd.Timedelta(days=30) + next_date) <= end_date:\n",
    "        \n",
    "        print(\"###################:\", next_date)\n",
    "        print('loop:',i)\n",
    "        unique_currencies1 = (sliced_df.loc[pd.IndexSlice[:, next_date], :].dropna()).index.get_level_values('Currency').unique()\n",
    "        next_date1 = pd.Timedelta(days=30) + next_date\n",
    "        unique_currencies2 = (sliced_df.loc[pd.IndexSlice[:, next_date1], :].dropna()).index.get_level_values('Currency').unique()\n",
    "        \n",
    "        # 将两个 unique_currencies Pandas Index 对象转换为 set\n",
    "        set_unique_currencies1 = set(unique_currencies1)\n",
    "        set_unique_currencies2 = set(unique_currencies2)\n",
    "        \n",
    "        # 找到两个集合中的共有元素\n",
    "        common_currencies = set_unique_currencies1.intersection(set_unique_currencies2)\n",
    "        \n",
    "        unique_currencies = pd.Index(list(common_currencies)).sort_values()\n",
    "        \n",
    "        # Slice and clean data\n",
    "        \n",
    "        sliced_df_scaled2 = sliced_df.loc[pd.IndexSlice[unique_currencies, start_date:next_date], :].sort_index().copy()\n",
    "        return_series_oos = sliced_returns.loc[pd.IndexSlice[unique_currencies, start_date:next_date]].sort_index().copy()\n",
    "        \n",
    "        non_nan_ind = ~np.any(np.isnan(sliced_df_scaled2), axis=1)\n",
    "        #sliced_df_scaled2[non_nan_ind] = minmax_scaler.fit_transform(sliced_df_scaled2[non_nan_ind])\n",
    "        \n",
    "        X = sliced_df_scaled2\n",
    "        y = return_series_oos\n",
    "        regr = InstrumentedPCA(n_factors=i, intercept=False, max_iter=5000,n_jobs=-1,iter_tol=0.0005)\n",
    "        regr = regr.fit(X=X, y=y)\n",
    "        \n",
    "        # data for prediction next day\n",
    "        sliced_df_scaled2 = sliced_df.loc[pd.IndexSlice[unique_currencies, start_date:next_date1], :].sort_index().copy()\n",
    "        return_series_oos = sliced_returns.loc[pd.IndexSlice[unique_currencies, start_date:next_date1]].sort_index().copy()\n",
    "\n",
    "        # scale X\n",
    "        non_nan_ind = ~np.any(np.isnan(sliced_df_scaled2), axis=1)\n",
    "        #sliced_df_scaled2[non_nan_ind] = minmax_scaler.transform(sliced_df_scaled2[non_nan_ind])\n",
    "        \n",
    "        X = sliced_df_scaled2\n",
    "        y = return_series_oos\n",
    "        y_returns = regr.predictOOS(X=X, y=y, mean_factor=False,n=k) # predict next periods\n",
    "        y_pred = y_returns[-1] # last prediction\n",
    "        \n",
    "        X_t = sliced_df_scaled2.loc[pd.IndexSlice[unique_currencies, next_date],:].sort_index()\n",
    "        \n",
    "        y_predm1, e_ft1 = forecastreturn(regr, X_t, n=k)\n",
    "        y_predm2, e_ft1 = forecastSVM(regr, X_t, n=k)\n",
    "        y_predm3, e_ft1 = forecastFactor(regr, X_t, n=k)\n",
    " \n",
    "\n",
    "        #non_nan_ind = ~np.any(np.isnan(X), axis=1)\n",
    "        y = sliced_returns.loc[pd.IndexSlice[unique_currencies, next_date1]].sort_index().values\n",
    "        \n",
    "        \n",
    "        ###########################\n",
    "        portfolio_return = np.nan\n",
    "        \n",
    "        if len(unique_currencies) < 20:\n",
    "            # Handle the special case when portfolio_currencies is empty\n",
    "            print(\"No suitable currencies found for portfolio for date:\", next_date1)\n",
    "        else:         \n",
    "            cov_matrix = calculate_cov(regr, X_t, window_size=30)\n",
    "            #expected_returns = [y_pred[i] for i in indices]\n",
    "            expected_returns = y_predm1\n",
    "            \n",
    "            optimal_weights, sharpe_ratio = optimize_portfolio_sharpe(expected_returns, cov_matrix, risk_budget=None, min_weight=0.01, max_weight=0.2)\n",
    "            predicted_values_df.loc[(unique_currencies, next_date1), 'optimal_weights'] = optimal_weights\n",
    "            portfolio_return = np.sum(y*predicted_values_df.loc[(unique_currencies, next_date1), 'optimal_weights'])\n",
    "            print(portfolio_return)\n",
    "        predicted_values_df.loc[(unique_currencies, next_date1), 'portfolio_return1'] = portfolio_return\n",
    "        \n",
    "        #########################\n",
    "        portfolio_return = np.nan\n",
    "        \n",
    "        if len(unique_currencies) < 20:\n",
    "            # Handle the special case when portfolio_currencies is empty\n",
    "            print(\"No suitable currencies found for portfolio for date:\", next_date1)\n",
    "        else:         \n",
    "            cov_matrix = calculate_cov(regr, X_t, window_size=30)\n",
    "            #expected_returns = [y_pred[i] for i in indices]\n",
    "            expected_returns = y_predm2\n",
    "            \n",
    "            optimal_weights, sharpe_ratio = optimize_portfolio_sharpe(expected_returns, cov_matrix, risk_budget=None, min_weight=0.01, max_weight=0.2)\n",
    "            predicted_values_df.loc[(unique_currencies, next_date1), 'optimal_weights'] = optimal_weights\n",
    "            portfolio_return = np.sum(y*predicted_values_df.loc[(unique_currencies, next_date1), 'optimal_weights'])\n",
    "            print(portfolio_return)\n",
    "        predicted_values_df.loc[(unique_currencies, next_date1), 'portfolio_return2'] = portfolio_return\n",
    "        \n",
    "        #################\n",
    "        portfolio_return = np.nan\n",
    "        \n",
    "        if len(unique_currencies) < 20:\n",
    "            # Handle the special case when portfolio_currencies is empty\n",
    "            print(\"No suitable currencies found for portfolio for date:\", next_date1)\n",
    "        else:         \n",
    "            cov_matrix = calculate_cov(regr, X_t, window_size=30)\n",
    "            #expected_returns = [y_pred[i] for i in indices]\n",
    "            expected_returns = y_predm3\n",
    "            \n",
    "            optimal_weights, sharpe_ratio = optimize_portfolio_sharpe(expected_returns, cov_matrix, risk_budget=None, min_weight=0.01, max_weight=0.2)\n",
    "            predicted_values_df.loc[(unique_currencies, next_date1), 'optimal_weights'] = optimal_weights\n",
    "            portfolio_return = np.sum(y*predicted_values_df.loc[(unique_currencies, next_date1), 'optimal_weights'])\n",
    "            print(portfolio_return)\n",
    "        predicted_values_df.loc[(unique_currencies, next_date1), 'portfolio_return3'] = portfolio_return\n",
    "    \n",
    "        \n",
    "        sum_diff += np.sum((y - y_pred)**2)\n",
    "        sum_diff_pre1 += np.sum((y - y_predm1)**2)\n",
    "        sum_diff_pre2 += np.sum((y - y_predm2)**2)\n",
    "        sum_diff_pre3 += np.sum((y - y_predm3)**2)\n",
    "        sum_r2 += np.sum(y ** 2)\n",
    "        \n",
    "        print(1 - (sum_diff/sum_r2))\n",
    "        print(1 - (sum_diff_pre1/sum_r2))\n",
    "        print(\"###################\")\n",
    "          \n",
    "        next_date += pd.Timedelta(days=step_size)\n",
    "    \n",
    "                           \n",
    "    results_oos_month_rank_full_arima[i] = {'model': regr}\n",
    "    results_oos_month_rank_full_arima[i]['r2_total'] = 1 - (sum_diff/sum_r2)\n",
    "    results_oos_month_rank_full_arima[i]['r2_pred1'] = 1 - (sum_diff_pre1/sum_r2)\n",
    "    results_oos_month_rank_full_arima[i]['r2_pred2'] = 1 - (sum_diff_pre2/sum_r2)\n",
    "    results_oos_month_rank_full_arima[i]['r2_pred3'] = 1 - (sum_diff_pre3/sum_r2)\n",
    "    results_oos_month_rank_full_arima[i]['pred'] = predicted_values_df\n",
    "    results_oos_month_rank_full_arima[i]['sharpe'] = portfolio_sharpe_ratio_df\n",
    "    results_oos_month_rank_full_arima[i]['port_ret1'] = predicted_values_df['portfolio_return1'].mean()\n",
    "    results_oos_month_rank_full_arima[i]['port_ret2'] = predicted_values_df['portfolio_return2'].mean()\n",
    "    results_oos_month_rank_full_arima[i]['port_ret3'] = predicted_values_df['portfolio_return3'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04780854-e591-4126-93da-f53f9c1a3122",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "predicted_values_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "090e66d0-175a-4815-8b61-0814d972033b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 保存词典\n",
    "with open('monthly_oos_rank_slice_svm.pickle', 'wb') as handle:\n",
    "    pickle.dump(results_oos_month_rank_slice_svm, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec842815-58a3-42a4-bce6-a28d9e7a66ba",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "results_oos_month_rank_slice_arima[1]['r2_total']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70959894-e04e-426e-aa5f-cb59ebb9d436",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "results_oos_month_rank_slice_arima[6]['r2_pred']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f84dc9f2-4c71-4352-bb9c-3935a89d00ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "Gamma, Factors = results_oos[4]['model'].get_factors(label_ind=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9158172-ce2c-4d3b-ba5e-c60c71e53b98",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "scaler1 = RobustScaler()\n",
    "scaler2 = MinMaxScaler()\n",
    "df2 = pd.DataFrame(scaler2.fit_transform(monthly_independ_variable_df), index=monthly_independ_variable_df.index)\n",
    "df2.describe().to_csv('11.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adc80f12-99e9-4b72-b75c-db8959d7e9a0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 加载词典\n",
    "with open('monthly_oos_rank_slice_svm.pickle', 'rb') as handle:\n",
    "    results_oos_month_rank_slice_arima = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9c596bd-4fdf-4976-900f-3772fdf98273",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 删除 NaN\n",
    "optimal_weights = results_oos_month_rank_slice_arima[6]['pred']['optimal_weights'].dropna()\n",
    "excess_returns = all_monthly_returns.loc[pd.IndexSlice[:, 'monthly Return']].dropna()\n",
    "\n",
    "# 对齐索引\n",
    "aligned_weights, aligned_returns = optimal_weights.align(excess_returns, join='inner')\n",
    "\n",
    "# 计算投资组合收益\n",
    "portfolio_return = aligned_weights * aligned_returns\n",
    "\n",
    "# 如果你想要每个时间点的投资组合总收益\n",
    "portfolio_return_by_date = (portfolio_return.reset_index().groupby('Date').sum()).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abec1891-f218-4fe0-b390-a17ae99fa307",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "portfolio_return_by_date.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9c3c8e1-445e-424c-812d-c6f00663d544",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "results_oos_month_rank_slice_arima[1]['port_ret']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe9486b0-9a04-4037-a0a7-a2014eb58334",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df= portfolio_return_by_date\n",
    "\n",
    "# 绘制图形\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(df.index, df[0], marker='o', linestyle='-')\n",
    "plt.title('Portfolio return')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Value')\n",
    "plt.grid(True)\n",
    "\n",
    "# 保存图像\n",
    "plt.savefig('portfolio_return.png')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b39692ba-131c-4022-80ef-759049506c43",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df= portfolio_sharpe_ratio_df\n",
    "\n",
    "# 绘制图形\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(df.index, df['sharpe_ratio'], marker='o')\n",
    "plt.title('sharpe_ratio')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Value')\n",
    "plt.grid(True)\n",
    "\n",
    "# 保存图像\n",
    "plt.savefig('sharpe_ratio.png')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "654de9c7-458a-4e5a-ba2e-edf8bf9bea3c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for i in range(3,4):\n",
    "    \n",
    "    predicted_values_df = results_oos[i]['pred'].dropna()\n",
    "    selected_indices = predicted_values_df.index\n",
    "    \n",
    "    extracted_returns = top_50_currencies_df.loc[pd.IndexSlice[selected_indices],:]\n",
    "    \n",
    "    predicted_values = predicted_values_df['predicted_value'].values\n",
    "    actual_values = extracted_returns['return'].values\n",
    "\n",
    "    statistic, pvalue = ks_2samp(predicted_values, actual_values)\n",
    "    \n",
    "    results_oos[i]['ks_2samp_pvalue'] = pvalue\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86d28982-76b8-4279-8819-9f767b6feb88",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "non_nan_ind = ~np.any(np.isnan(depend_variable_df), axis=1)\n",
    "slice_top_50 = depend_variable_df[non_nan_ind]\n",
    "\n",
    "# 按照 'Currency' 进行分组，并计算 'active_addresses' 的平均值\n",
    "grouped_df = slice_top_50.groupby('Currency')['active_addresses'].mean()\n",
    "top_50_active_addresses = grouped_df.sort_values(ascending=False).head(35)\n",
    "\n",
    "# 按照 'Currency' 进行分组，并计算 'active_addresses' 的平均值\n",
    "grouped_df = slice_top_50.groupby('Currency').size()\n",
    "top_50_history = grouped_df.sort_values(ascending=False).head(25)\n",
    "\n",
    "grouped_df = slice_top_50.groupby('Currency')['size'].mean()\n",
    "top_50_size = grouped_df.sort_values(ascending=False).head(35)\n",
    "\n",
    "# 将两个 unique_currencies Pandas Index 对象转换为 set\n",
    "set_unique_currencies1 = set(top_50_size.index.get_level_values('Currency'))\n",
    "set_unique_currencies2 = set(top_50_active_addresses.index.get_level_values('Currency'))\n",
    "set_unique_currencies3 = set(top_50_history.index.get_level_values('Currency'))\n",
    "        \n",
    "# 找到两个集合中的共有元素\n",
    "top_currencies = set_unique_currencies2.intersection(set_unique_currencies1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a7f3b58-cd24-4239-bb44-e93b5181fe0f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "len(top_currencies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93632ca3-664b-4bc1-b4a4-a382d73d4aed",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "depend_variable_df_sliced = monthly_independ_variable_df[monthly_independ_variable_df.index.get_level_values('Currency').isin(top_currencies)]\n",
    "\n",
    "top_50_currencies_df_sliced = all_monthly_returns[all_monthly_returns.index.get_level_values('Currency').isin(top_currencies)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1abd001-1496-45b9-9b54-93a28db167b5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 重置索引以便可以进行过滤操作\n",
    "reset_df = depend_variable_df_sliced.reset_index()\n",
    "sliced_df = reset_df.set_index(['Currency', 'Date'])\n",
    "\n",
    "reset_df = top_50_currencies_df_sliced.reset_index()\n",
    "sliced_returns= reset_df.set_index(['Currency', 'Date'])['excess_return']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25cc296f-c55b-4d01-9301-7cb8b868ec47",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Step 1: Filter for rows corresponding to 'BTC' the longest coin\n",
    "valid_df = sliced_df.loc['BTC'].copy()\n",
    "\n",
    "# Step 2: Determine the first time a non-NaN value appears for each column\n",
    "first_valid_indices = valid_df.apply(lambda col: col.first_valid_index())\n",
    "\n",
    "idx = max(first_valid_indices)\n",
    "\n",
    "first_valid_idx = idx\n",
    "\n",
    "window_size = 18*30\n",
    "step_size = 1*30\n",
    "#valid_date = 60\n",
    "\n",
    "results1_oos = {}\n",
    "\n",
    "# Initialize the scaler\n",
    "minmax_scaler = MinMaxScaler(feature_range=(-0.5, 0.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78c2d53c-343e-4668-83cd-e0b0b6aacac7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for i in [1,3,6]:  # for n_factors from 1 to 6\n",
    "    sum_diff = 0\n",
    "    sum_diff_pre = 0\n",
    "    sum_r2 = 0\n",
    "    \n",
    "    \n",
    "    start_date = first_valid_idx\n",
    "    next_date = pd.Timedelta(days=window_size) + start_date\n",
    "    end_date = monthly_independ_variable_df.index.get_level_values('Date').max()\n",
    "    next_date1 = pd.Timedelta(days=30) + next_date\n",
    "    \n",
    "    predicted_values_df = pd.DataFrame(index=sliced_df.index, columns=['predicted_value'])\n",
    "    portfolio_sharpe_ratio_df = pd.DataFrame(index=all_monthly_rf.index, columns=['sharpe_ratio'])\n",
    "\n",
    "    while next_date <= end_date and (pd.Timedelta(days=30) + next_date) <= end_date:\n",
    "        \n",
    "        print(\"###################:\", next_date)\n",
    "        unique_currencies1 = (sliced_df.loc[pd.IndexSlice[:, next_date], :].dropna()).index.get_level_values('Currency').unique()\n",
    "        next_date1 = pd.Timedelta(days=30) + next_date\n",
    "        unique_currencies2 = (sliced_df.loc[pd.IndexSlice[:, next_date1], :].dropna()).index.get_level_values('Currency').unique()\n",
    "        \n",
    "        # 将两个 unique_currencies Pandas Index 对象转换为 set\n",
    "        set_unique_currencies1 = set(unique_currencies1)\n",
    "        set_unique_currencies2 = set(unique_currencies2)\n",
    "        \n",
    "        # 找到两个集合中的共有元素\n",
    "        common_currencies = set_unique_currencies1.intersection(set_unique_currencies2)\n",
    "        \n",
    "        unique_currencies = pd.Index(list(common_currencies)).sort_values()\n",
    "        \n",
    "        \n",
    "        # Slice and clean data\n",
    "        \n",
    "        sliced_df_scaled2 = sliced_df.loc[pd.IndexSlice[unique_currencies, start_date:next_date], :].sort_index().copy()\n",
    "        return_series_oos = sliced_returns.loc[pd.IndexSlice[unique_currencies, start_date:next_date]].sort_index().copy()\n",
    "        \n",
    "        non_nan_ind = ~np.any(np.isnan(sliced_df_scaled2), axis=1)\n",
    "        sliced_df_scaled2[non_nan_ind] = minmax_scaler.fit_transform(sliced_df_scaled2[non_nan_ind])\n",
    "        \n",
    "        X = sliced_df_scaled2\n",
    "        y = return_series_oos\n",
    "        regr = InstrumentedPCA(n_factors=i, intercept=False, max_iter=5000,n_jobs=-1,iter_tol=0.0005)\n",
    "        regr = regr.fit(X=X, y=y)\n",
    "        \n",
    "        # data for prediction next day\n",
    "        sliced_df_scaled2 = sliced_df.loc[pd.IndexSlice[unique_currencies, start_date:next_date1], :].sort_index().copy()\n",
    "        return_series_oos = sliced_returns.loc[pd.IndexSlice[unique_currencies, start_date:next_date1]].sort_index().copy()\n",
    "\n",
    "        # scale X\n",
    "        non_nan_ind = ~np.any(np.isnan(sliced_df_scaled2), axis=1)\n",
    "        sliced_df_scaled2[non_nan_ind] = minmax_scaler.transform(sliced_df_scaled2[non_nan_ind])\n",
    "        \n",
    "        X = sliced_df_scaled2\n",
    "        y = return_series_oos\n",
    "        y_returns = regr.predictOOS(X=X, y=y, mean_factor=False,n=5) # predict next periods\n",
    "        y_pred = y_returns[-1] # last prediction\n",
    "        \n",
    "        X_t = sliced_df_scaled2.loc[pd.IndexSlice[unique_currencies, next_date],:].sort_index()\n",
    "        y_predm, values = forecastFactor(regr, X_t, n=5)\n",
    "        \n",
    "\n",
    "        #non_nan_ind = ~np.any(np.isnan(X), axis=1)\n",
    "        y = sliced_returns.loc[pd.IndexSlice[unique_currencies, next_date1]].sort_index().values\n",
    "        \n",
    "        #non_nan_count = np.sum(~np.isnan(y_returns), axis=0)\n",
    "        #indices = np.where(non_nan_count > 20)[0]\n",
    "        #portfolio_currencies = [unique_currencies[i] for i in indices]\n",
    "        \n",
    "        print('##################')\n",
    "        print(len(unique_currencies))\n",
    "        print(np.isnan(y_predm).all()==False)\n",
    "        \n",
    "        portfolio_return = np.nan\n",
    "        \n",
    "        if len(unique_currencies) < 20:\n",
    "            # Handle the special case when portfolio_currencies is empty\n",
    "            print(\"No suitable currencies found for portfolio for date:\", next_date1)\n",
    "        else:         \n",
    "            cov_matrix = calculate_cov(regr, X_t, window_size=15)\n",
    "            #expected_returns = [y_pred[i] for i in indices]\n",
    "            expected_returns = y_predm\n",
    "            \n",
    "            optimal_weights, sharpe_ratio = optimize_portfolio_sharpe(expected_returns, cov_matrix, risk_budget=None, min_weight=0.01, max_weight=0.2)\n",
    "            predicted_values_df.loc[(unique_currencies, next_date1), 'optimal_weights'] = optimal_weights\n",
    "            portfolio_sharpe_ratio_df.loc[next_date1, 'sharpe_ratio'] = sharpe_ratio\n",
    "            portfolio_return = np.sum(y*predicted_values_df.loc[(unique_currencies, next_date1), 'optimal_weights'])\n",
    "            \n",
    "        print(portfolio_return)\n",
    "        \n",
    "        predicted_values_df.loc[(unique_currencies, next_date1), 'predicted_value'] = y_pred\n",
    "        predicted_values_df.loc[(unique_currencies, next_date1), 'predicted_mean_value'] = y_predm\n",
    "        predicted_values_df.loc[(unique_currencies, next_date1), 'portfolio_return'] = portfolio_return\n",
    "    \n",
    "        \n",
    "        sum_diff += np.sum((y - y_pred)**2)\n",
    "        sum_diff_pre += np.sum((y - y_predm)**2)\n",
    "        sum_r2 += np.sum(y ** 2)\n",
    "        \n",
    "        print(1 - (sum_diff/sum_r2))\n",
    "        print(1 - (sum_diff_pre/sum_r2))\n",
    "        print(\"###################\")\n",
    "        \n",
    "        \n",
    "        \n",
    "        next_date += pd.Timedelta(days=step_size)\n",
    "    \n",
    "                           \n",
    "    results1_oos[i] = {'model': regr}\n",
    "    results1_oos[i]['r2_total'] = 1 - (sum_diff/sum_r2)\n",
    "    results1_oos[i]['r2_pred'] = 1 - (sum_diff_pre/sum_r2)\n",
    "    results1_oos[i]['pred'] = predicted_values_df\n",
    "    results1_oos[i]['sharpe'] = portfolio_sharpe_ratio_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7d331bb-dd9b-4ccc-90e6-9c1d79d2a029",
   "metadata": {},
   "source": [
    "#### show output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25e5b56d-6df9-4570-87ad-0bfd209ae02a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 初始化 LaTeX 表格字符串\n",
    "latex_table = r\"\\begin{table}\"\n",
    "latex_table += r\"\\centering\"\n",
    "latex_table += r\"\\caption{Sample from October 2017 to March 2020}\"\n",
    "latex_table += r\"\\begin{tabular}{lccccc}\"\n",
    "latex_table += r\"\\hline\"\n",
    "latex_table += r\"\\multicolumn{7}{c}{IPCA \\(\\left(\\Gamma_\\alpha=0\\right)\\)} \\\\\"\n",
    "latex_table += r\"\\hline\"\n",
    "latex_table += r\"& IPCA1 & IPCA2 & IPCA3 & IPCA4 & IPCA5 \\\\\"\n",
    "latex_table += r\"\\cline{2-7}\"\n",
    "\n",
    "# 添加具体数据\n",
    "r2_total = r\"\\(R_{\\text{tot}}^2(\\%)\\) & \" + \" & \".join([f\"{results_oos[i]['r2_total']:.3f}\" for i in range(1, 6)])\n",
    "r2_pred = r\"\\(R_{\\text{pred}}^2(\\%)\\) & \" + \" & \".join([f\"{results_oos[i]['r2_pred']:.3f}\" for i in range(1, 6)])\n",
    "RPE = r\"\\(RPE\\) & \" + \" & \".join([f\"{results_oos[i]['RPE']:.3f}\" for i in range(1, 6)])\n",
    "\n",
    "latex_table += f\"{r2_total} \\\\\\\\\"\n",
    "latex_table += f\"{r2_pred} \\\\\\\\\"\n",
    "latex_table += f\"{RPE} \\\\\\\\\"\n",
    "\n",
    "latex_table += r\"\\hline\"\n",
    "latex_table += r\"\\end{tabular}\"\n",
    "latex_table += r\"\\end{table}\"\n",
    "\n",
    "# 打印 LaTeX 表格\n",
    "print(latex_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8989d16-ed34-4c2c-a058-381392866773",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 创建一个目录来保存图像，如果它还不存在的话\n",
    "output_directory = \"monthly_excess_return_currency_plots_oos\"\n",
    "if not os.path.exists(output_directory):\n",
    "    os.makedirs(output_directory)\n",
    "    \n",
    "# 找到r2_total最大值的索引i\n",
    "max_i = max(results_oos.keys(), key=lambda k: results_oos[k]['r2_total'])\n",
    "\n",
    "print(max_i)\n",
    "# 使用找到的i来获取'pred'\n",
    "predicted_values_df = results_oos[max_i]['pred'].dropna()\n",
    "\n",
    "unique_currencies = predicted_values_df.index.get_level_values('Currency').unique()\n",
    "\n",
    "for currency in unique_currencies:\n",
    "    fig, ax = plt.subplots(figsize=(12, 6))\n",
    "\n",
    "    # Plot predicted values for the current currency\n",
    "    #predicted_values_df.loc[pd.IndexSlice[currency, 'predicted_mean_value']].plot(ax=ax, label='Predicted mean returns', color='blue')\n",
    "\n",
    "    # Plot extracted returns for the current currency\n",
    "    #all_monthly_returns.loc[pd.IndexSlice[currency, 'monthly Return']].plot(ax=ax, label='monthly Return', color='red')\n",
    "    sliced_returns.loc[pd.IndexSlice[currency, 'monthly Return']].plot(ax=ax, label='Excess returns', color='red')\n",
    "    \n",
    "    # Plot extracted returns for the current currency\n",
    "    predicted_values_df.loc[pd.IndexSlice[currency, 'predicted_value']].plot(ax=ax, label='Predicted returns', color='green')\n",
    "\n",
    "    ax.set_title(f\"Time Series for {currency}\")\n",
    "    ax.legend()\n",
    "\n",
    "    # Save the figure to the desired directory\n",
    "    file_name = os.path.join(output_directory, f\"{currency}_plot.png\")\n",
    "    plt.savefig(file_name)\n",
    "\n",
    "    # Optionally, you can show the plot as well if you want to inspect them\n",
    "    # plt.show()\n",
    "\n",
    "    # Close the plot to free up memory\n",
    "    plt.close(fig)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my_backup_environment",
   "language": "python",
   "name": "my_backup_environment"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
